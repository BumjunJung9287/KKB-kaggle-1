{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"submission.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"vGJGsJfsggSr","colab_type":"code","outputId":"12d2d5b4-ca2d-47eb-ce54-c2e082be1ded","executionInfo":{"status":"ok","timestamp":1584024818677,"user_tz":-540,"elapsed":661,"user":{"displayName":"bumjun jeong","photoUrl":"","userId":"05714213050548556605"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["from google.colab import drive\n","drive.mount('/content/gdrive/')\n","%cd gdrive/My\\ Drive/KKB-kaggle/bengaliai-cv19/notebooks"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive/; to attempt to forcibly remount, call drive.mount(\"/content/gdrive/\", force_remount=True).\n","/content/gdrive/My Drive/KKB-kaggle/bengaliai-cv19/notebooks\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"U67CYVMTg7tE","colab_type":"code","colab":{}},"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load in \n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the \"../input/\" directory.\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# Any results you write to the current directory are saved as output."],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yNVEBroeewc4","colab_type":"code","outputId":"d6df1748-f879-4ed7-b558-1cfe6a677852","executionInfo":{"status":"ok","timestamp":1584024827548,"user_tz":-540,"elapsed":5222,"user":{"displayName":"bumjun jeong","photoUrl":"","userId":"05714213050548556605"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["!pip install efficientnet_pytorch"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: efficientnet_pytorch in /usr/local/lib/python3.6/dist-packages (0.6.3)\n","Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from efficientnet_pytorch) (1.4.0)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"TqMWjaRU2KU4","colab_type":"code","colab":{}},"source":["## 諸々の import\n","\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import recall_score\n","import cv2\n","# from tqdm.auto import tqdm\n","import copy\n","import datetime\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader\n","from torchvision import datasets, transforms, utils\n","import torchvision.models as models\n","from torch.autograd import Variable\n","import torch.optim as optim\n","import torchvision\n","import PIL\n","# from torchsummary import summary\n","import gc"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RbsZSbkg9SrB","colab_type":"code","colab":{}},"source":["## Parameters\n","\n","# resize後のサイズ\n","HEIGHT = 64\n","WIDTH = 64\n","\n","# 画像を3次元にするかどうか（EfficientNetなどを使うときはTrue）\n","enable_3d = True\n","\n","# True なら Cross Validation を実施する\n","# Kaggle に提出するデータを作るときは False にしてください\n","do_validation = False\n","\n","# True なら submission.csv を生成する\n","create_submission = False\n","\n","val_perc = 0.2  # validation set の割合（クロスバリデーション）\n","\n","#epochs number\n","epochs = 30\n","\n","#初期値として、保存した重みを使う時はこれをTrueに\n","load_flag = True\n","\n","#Kaggleで提出するときはTrueにする\n","kaggle_flag = False\n","\n","#loadするファイルへのパス\n","if kaggle_flag:\n","    model_path = '/kaggle/input/model_load/resnet18_epoch1_2020-03-09_14-58-43.pth'\n","else:\n","    model_path = '../trained_models/efficientnetb1_epoch57_2020-03-12_23-41-59.pth'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"f2uk6pfBBxdy","colab_type":"code","colab":{}},"source":["if kaggle_flag:\n","    dataset_dir = '/kaggle/input/bengaliai-cv19'\n","    model_dir = '/kaggle/input/trained_models'\n","else:\n","    dataset_dir = '../dataset'\n","    model_dir = '../trained_models'\n","\n","train_df = pd.read_csv(dataset_dir + '/train.csv')\n","test_df = pd.read_csv(dataset_dir + '/test.csv')\n","class_map_df = pd.read_csv(dataset_dir + '/class_map.csv')\n","sample_sub_df = pd.read_csv(dataset_dir + '/sample_submission.csv')\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RU66nFHqgj7K","colab_type":"code","outputId":"502ead03-89db-4994-9441-9a8b86534bc1","executionInfo":{"status":"ok","timestamp":1584025400655,"user_tz":-540,"elapsed":2090,"user":{"displayName":"bumjun jeong","photoUrl":"","userId":"05714213050548556605"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["#モデルの設定\n","\n","from preprocess import *\n","from save_load import *\n","#from model.CNN import model\n","#from model.efficientnet import model\n","from model.efficientnetb1 import model\n","#from model.resnet18 import model\n","#from model.resnet34 import model\n","#from model.resnet50 import model\n","#from model.resnet101 import model\n","#from model.resnet152 import model\n","\n","model = model()\n","model = try_gpu(model)\n","optimizer = optim.Adam(model.parameters())\n","if load_flag:\n","    model,optimizer,start_epoch = load_model(model,optimizer,model_path)\n","    model = try_gpu(model)\n","    start_epoch += 1\n","else:\n","    start_epoch = 1    \n","criterion1 = nn.CrossEntropyLoss() \n","# optimizer = optim.Adam(model.parameters(), lr=0.0001) # epoch 23 までは0.001\n","# optimizer = optim.Adam(model.parameters(), lr=0.00001) # epoch 52まで0.0001\n","optimizer = optim.Adam(model.parameters(), lr=0.0001) # epoch 57からまた0.0001"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Loaded pretrained weights for efficientnet-b1\n","loaded from ../trained_models/efficientnetb1_epoch57_2020-03-12_23-41-59.pth\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Vket_LbeGIcv","colab_type":"code","colab":{}},"source":["# ##もし今までのmodelの重みのみを保存しているファイルから読み込む場合\n","# device = torch.device('cpu')\n","# model.load_state_dict(torch.load(model_dir+\"/resnet18.pth\",map_location=device)) #各自パスを設定\n","# model = try_gpu(model)\n","# start_epoch = 31 #各自変更"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"h7G-i0XLtsbY","colab_type":"code","colab":{}},"source":["## train関数、test関数\n","\n","def train(model, epoch, train_loader):\n","    model.train()\n","    \n","    size = len(train_loader.dataset)\n","    pred_r, pred_v, pred_c = np.zeros(size), np.zeros(size), np.zeros(size)\n","    true_r, true_v, true_c = np.zeros(size), np.zeros(size), np.zeros(size)\n","    index = 0\n","\n","    for data in train_loader:\n","        inputs, root_l, vowel_l, consonant_l = data\n","        inputs, root_l, vowel_l, consonant_l = Variable(inputs), Variable(root_l), Variable(vowel_l), Variable(consonant_l)\n","        inputs, root_l, vowel_l, consonant_l = try_gpu(inputs), try_gpu(root_l), try_gpu(vowel_l), try_gpu(consonant_l)\n","        optimizer.zero_grad()\n","        root_o, vowel_o, consonant_o = model(inputs)\n","        root_pred, vowel_pred, consonant_pred = torch.max(root_o.data,1)[1], torch.max(vowel_o.data,1)[1], torch.max(consonant_o.data,1)[1]\n","        loss1 = criterion1(root_o, root_l)\n","        loss2 = criterion1(vowel_o, vowel_l)\n","        loss3 = criterion1(consonant_o, consonant_l)\n","        (loss1+loss2+loss3).backward()\n","        optimizer.step()\n","        for i in range(inputs.size(0)):\n","            pred_r[index] = root_pred[i]\n","            pred_v[index] = vowel_pred[i]\n","            pred_c[index] = consonant_pred[i]\n","            true_r[index] = root_l[i]\n","            true_v[index] = vowel_l[i]\n","            true_c[index] = consonant_l[i]\n","            index += 1\n","    recall_r = recall_score(true_r, pred_r, average='macro')\n","    recall_v = recall_score(true_v, pred_v, average='macro')\n","    recall_c = recall_score(true_c, pred_c, average='macro')\n","    final_score = (2.*recall_r + recall_v + recall_c) / 4.\n","\n","    print(f'Root Recall(train): {recall_r:.5f}')\n","    print(f'Vowel Recall(train): {recall_v:.5f}')\n","    print(f'Consonant Recall(train): {recall_c:.5f}')\n","    print(f'Score(train): {final_score:.5f}')\n","   \n","\n","def test(model, test_loader):\n","    model.eval()\n","\n","    size = len(test_loader.dataset)\n","    pred_r, pred_v, pred_c = np.zeros(size), np.zeros(size), np.zeros(size)\n","    true_r, true_v, true_c = np.zeros(size), np.zeros(size), np.zeros(size)\n","    index = 0\n","    \n","    for data in test_loader:\n","        inputs, root_l, vowel_l, consonant_l = data\n","        inputs, root_l, vowel_l, consonant_l = Variable(inputs), Variable(root_l), Variable(vowel_l), Variable(consonant_l)\n","        inputs, root_l, vowel_l, consonant_l = try_gpu(inputs), try_gpu(root_l), try_gpu(vowel_l), try_gpu(consonant_l)\n","        \n","        root_o, vowel_o, consonant_o = model(inputs) \n","        root_pred, vowel_pred, consonant_pred = torch.max(root_o.data,1)[1], torch.max(vowel_o.data,1)[1], torch.max(consonant_o.data,1)[1]\n","        for i in range(inputs.size(0)):\n","            pred_r[index] = root_pred[i]\n","            pred_v[index] = vowel_pred[i]\n","            pred_c[index] = consonant_pred[i]\n","            true_r[index] = root_l[i]\n","            true_v[index] = vowel_l[i]\n","            true_c[index] = consonant_l[i]\n","            index += 1\n","\n","    recall_r = recall_score(true_r, pred_r, average='macro')\n","    recall_v = recall_score(true_v, pred_v, average='macro')\n","    recall_c = recall_score(true_c, pred_c, average='macro')\n","    final_score = (2.*recall_r + recall_v + recall_c) / 4.\n","\n","    print(f'Root Recall(test): {recall_r:.5f}')\n","    print(f'Vowel Recall(test): {recall_v:.5f}')\n","    print(f'Consonant Recall(test): {recall_c:.5f}')\n","    print(f'Score(test): {final_score:.5f}')\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5wZ0lQFJ3Ajz","colab_type":"code","outputId":"652b8d68-6fa2-435a-8bd2-cf63ce231efb","executionInfo":{"status":"ok","timestamp":1584025265898,"user_tz":-540,"elapsed":384072,"user":{"displayName":"bumjun jeong","photoUrl":"","userId":"05714213050548556605"}},"colab":{"base_uri":"https://localhost:8080/","height":238}},"source":["## データの読み込み\n","\n","X_all = np.empty((0, HEIGHT*WIDTH))\n","Y_root_all = np.empty((0, 168))\n","Y_vowel_all = np.empty((0, 11))\n","Y_cons_all = np.empty((0, 7))\n","\n","for parq_i in range(4):\n","    print(f'Parquet {parq_i} を読み込み中')\n","    train_df_with_img = pd.merge(pd.read_parquet(dataset_dir + f'/train_image_data_{parq_i}.parquet'), train_df, on='image_id').drop(['image_id'], axis=1)\n","    \n","    X = train_df_with_img.drop(columns=['grapheme_root', 'vowel_diacritic', 'consonant_diacritic', 'grapheme'])\n","    X_resized = resize(X, out_height=HEIGHT, out_width=WIDTH).astype(np.uint8) # astype(np.uint8)をしてあげることで後で cv2.cvtColor(out_data, cv2.COLOR_GRAY2RGB) が実行できるようになる\n","    \n","    Y_root = pd.get_dummies(train_df_with_img['grapheme_root']).values\n","    Y_vowel = pd.get_dummies(train_df_with_img['vowel_diacritic']).values\n","    Y_cons = pd.get_dummies(train_df_with_img['consonant_diacritic']).values\n","\n","    X_all = np.append(X_all, X_resized, axis=0)\n","    Y_root_all = np.append(Y_root_all, Y_root, axis=0)\n","    Y_vowel_all = np.append(Y_vowel_all, Y_vowel, axis=0)\n","    Y_cons_all = np.append(Y_cons_all, Y_cons, axis=0)\n","\n","    del X\n","    del X_resized\n","    del Y_root\n","    del Y_vowel \n","    del Y_cons \n","    gc.collect()\n","\n","print(X_all.shape)\n","print(Y_root_all.shape)\n","print(Y_vowel_all.shape)\n","print(Y_cons_all.shape)\n","\n","Y_all = [Y_root_all, Y_vowel_all, Y_cons_all]\n","\n","trainval_dataset = MyDataset(X_all, Y_all, enable_3d=enable_3d, H=HEIGHT, W=WIDTH)\n","\n","del X_all\n","del Y_root_all\n","del Y_vowel_all\n","del Y_cons_all\n","del Y_all\n","gc.collect()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Parquet 0 を読み込み中\n","Resizing raw image... / 前処理実行中…\n","Parquet 1 を読み込み中\n","Resizing raw image... / 前処理実行中…\n","Parquet 2 を読み込み中\n","Resizing raw image... / 前処理実行中…\n","Parquet 3 を読み込み中\n","Resizing raw image... / 前処理実行中…\n","(200840, 4096)\n","(200840, 168)\n","(200840, 11)\n","(200840, 7)\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["0"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"code","metadata":{"id":"ee2x8byvB8eW","colab_type":"code","outputId":"693073dc-1e52-49b1-bf83-1870f2a41883","executionInfo":{"status":"error","timestamp":1584024700611,"user_tz":-540,"elapsed":18590,"user":{"displayName":"bumjun jeong","photoUrl":"","userId":"05714213050548556605"}},"colab":{"base_uri":"https://localhost:8080/","height":561}},"source":["## 訓練ループ / Training Loop Kaggleでやる際には使わない\n","\n","print('==========================')\n","print('Starting training.')\n","if do_validation:\n","    n_samples = len(trainval_dataset)\n","    train_size = int(len(trainval_dataset)*(1.0 - val_perc))\n","    val_size = n_samples - train_size\n","    print(f'train size: {train_size}, validation size: {val_size}')\n","\n","    subset, val_dataset = torch.utils.data.random_split(trainval_dataset, [train_size, val_size])\n","\n","    if enable_3d:\n","        train_dataset = TransformDataset(subset, transform=transforms.RandomChoice(\n","            [transform_none, transform_crop224, transform_rotate, transform_noise]\n","        ))\n","\n","    else:\n","        train_dataset = TransformDataset(subset, transform=transforms.RandomChoice(\n","            [transform_none, transform_crop64, transform_rotate, transform_noise]\n","        ))\n","\n","    del trainval_dataset\n","    del subset\n","    gc.collect()\n","\n","    train_loader = DataLoader(dataset=train_dataset, batch_size=32, shuffle=True, num_workers=4)\n","    val_loader = DataLoader(dataset=val_dataset, batch_size=32, num_workers=0)\n","\n","    for i in range(start_epoch,epochs+start_epoch):\n","        print(f'Epoch number {i}')\n","        train(model, i, train_loader)\n","        test(model, val_loader)\n","        save_model(model,optimizer,model_dir, i)\n","\n","    # メモリ節約\n","    del train_dataset\n","    del val_dataset\n","    del train_loader\n","    del val_loader\n","    gc.collect()\n","\n","else:\n","    n_samples = len(trainval_dataset)\n","    print(f'train size: {n_samples}')\n","\n","    if enable_3d:\n","        trainval_dataset.transform = transforms.RandomChoice(\n","            [transform_none, transform_crop224, transform_rotate, transform_noise]\n","        )\n","    \n","    else:\n","        trainval_dataset.transform = transforms.RandomChoice(\n","            [transform_none, transform_crop64, transform_rotate, transform_noise]\n","        )\n","\n","    train_loader = DataLoader(dataset=trainval_dataset, batch_size=32, shuffle=True, num_workers=4)\n","\n","    for i in range(start_epoch,epochs+start_epoch):\n","        print(f'Epoch number {i}')\n","        train(model, i, train_loader)\n","        save_model(model,optimizer,model_dir, i)\n","\n","    # メモリ節約\n","    del trainval_dataset\n","    del train_loader\n","    gc.collect()\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["==========================\n","Starting training.\n","train size: 200840\n","Epoch number 58\n","Root Recall(train): 0.99528\n","Vowel Recall(train): 0.99725\n","Consonant Recall(train): 0.99645\n","Score(train): 0.99606\n","---saving model of epoch 58---\n","save finished\n","Epoch number 59\n","Root Recall(train): 0.99561\n","Vowel Recall(train): 0.99700\n","Consonant Recall(train): 0.99692\n","Score(train): 0.99629\n","---saving model of epoch 59---\n","save finished\n","Epoch number 60\n","Root Recall(train): 0.99596\n","Vowel Recall(train): 0.99692\n","Consonant Recall(train): 0.99713\n","Score(train): 0.99649\n","---saving model of epoch 60---\n","save finished\n","Epoch number 61\n","Root Recall(train): 0.99545\n","Vowel Recall(train): 0.99699\n","Consonant Recall(train): 0.99699\n","Score(train): 0.99622\n","---saving model of epoch 61---\n","save finished\n","Epoch number 62\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","outputId":"890f6d64-deac-4611-956f-0a99ebded2a5","executionInfo":{"status":"ok","timestamp":1583735471124,"user_tz":-540,"elapsed":1294535,"user":{"displayName":"Daisuke Nakajima","photoUrl":"","userId":"10532649301620693567"}},"id":"wMIW8MM2_yfF","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["## 提出ファイルの作成\n","\n","target=[]\n","row_id=[] # row_id place holder\n","\n","for parq_i in range(4):\n","    df_test_img = pd.read_parquet(dataset_dir + f'/test_image_data_{parq_i}.parquet')\n","    # df_test_img = pd.read_parquet(dataset_dir + f'/train_image_data_{parq_i}.parquet') # Error Check!\n","    df_test_img.set_index('image_id', inplace=True)\n","\n","    X_test_resized = resize(df_test_img, out_height=HEIGHT, out_width=WIDTH).astype(np.uint8)\n","\n","    for k, id in enumerate(df_test_img.index.values):\n","        X = X_test_resized[k]\n","\n","        if enable_3d:\n","            X = cv2.resize(X.reshape(HEIGHT, WIDTH), (224, 224),interpolation=cv2.INTER_AREA)\n","            X = X.reshape(224, 224, 1)\n","            X = cv2.cvtColor(X, cv2.COLOR_GRAY2RGB)\n","            X = np.transpose(X, (2,0,1)) / 255.0\n","            X = X.reshape(1, 3, 224, 224) \n","        \n","        else:\n","            X = X.reshape(1, 1, HEIGHT, WIDTH) / 255.0\n","\n","        test_input = torch.tensor(X, dtype=torch.float)\n","        test_input = Variable(test_input)\n","        test_input = try_gpu(test_input)\n","        \n","        model.eval()\n","        root_o, vowel_o, consonant_o = model(test_input)\n","        root_pred, vowel_pred, consonant_pred = torch.max(root_o.data,1)[1], torch.max(vowel_o.data,1)[1], torch.max(consonant_o.data,1)[1]\n","        if torch.cuda.is_available():\n","            root_pred, vowel_pred, consonant_pred = root_pred.to(torch.device(\"cpu\")), vowel_pred.to(torch.device(\"cpu\")), consonant_pred.to(torch.device(\"cpu\"))\n","\n","        root_pred, vowel_pred, consonant_pred = root_pred.item(), vowel_pred.item(), consonant_pred.item()\n","        \n","        row_id.append(id+'_consonant_diacritic')\n","        target.append(consonant_pred)\n","        row_id.append(id+'_grapheme_root')\n","        target.append(root_pred)\n","        row_id.append(id+'_vowel_diacritic')\n","        target.append(vowel_pred)\n","    \n","    del df_test_img\n","    del X_test_resized\n","    gc.collect()\n","\n","\n","df_sample = pd.DataFrame(\n","    {\n","        'row_id': row_id,\n","        'target':target\n","    },\n","    columns = ['row_id','target'] \n",")\n","\n","if create_submission:\n","    df_sample.to_csv('submission.csv',index=False)\n","\n","df_sample.head(36)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Resizing raw image... / 前処理実行中…\n","Resizing raw image... / 前処理実行中…\n","Resizing raw image... / 前処理実行中…\n","Resizing raw image... / 前処理実行中…\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>row_id</th>\n","      <th>target</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Test_0_consonant_diacritic</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Test_0_grapheme_root</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Test_0_vowel_diacritic</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Test_1_consonant_diacritic</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Test_1_grapheme_root</td>\n","      <td>93</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>Test_1_vowel_diacritic</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>Test_2_consonant_diacritic</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>Test_2_grapheme_root</td>\n","      <td>19</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>Test_2_vowel_diacritic</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>Test_3_consonant_diacritic</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>Test_3_grapheme_root</td>\n","      <td>115</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>Test_3_vowel_diacritic</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>Test_4_consonant_diacritic</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>Test_4_grapheme_root</td>\n","      <td>55</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>Test_4_vowel_diacritic</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>Test_5_consonant_diacritic</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>Test_5_grapheme_root</td>\n","      <td>115</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>Test_5_vowel_diacritic</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>Test_6_consonant_diacritic</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>Test_6_grapheme_root</td>\n","      <td>150</td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>Test_6_vowel_diacritic</td>\n","      <td>9</td>\n","    </tr>\n","    <tr>\n","      <th>21</th>\n","      <td>Test_7_consonant_diacritic</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>22</th>\n","      <td>Test_7_grapheme_root</td>\n","      <td>137</td>\n","    </tr>\n","    <tr>\n","      <th>23</th>\n","      <td>Test_7_vowel_diacritic</td>\n","      <td>7</td>\n","    </tr>\n","    <tr>\n","      <th>24</th>\n","      <td>Test_8_consonant_diacritic</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>25</th>\n","      <td>Test_8_grapheme_root</td>\n","      <td>119</td>\n","    </tr>\n","    <tr>\n","      <th>26</th>\n","      <td>Test_8_vowel_diacritic</td>\n","      <td>9</td>\n","    </tr>\n","    <tr>\n","      <th>27</th>\n","      <td>Test_9_consonant_diacritic</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>28</th>\n","      <td>Test_9_grapheme_root</td>\n","      <td>133</td>\n","    </tr>\n","    <tr>\n","      <th>29</th>\n","      <td>Test_9_vowel_diacritic</td>\n","      <td>10</td>\n","    </tr>\n","    <tr>\n","      <th>30</th>\n","      <td>Test_10_consonant_diacritic</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>31</th>\n","      <td>Test_10_grapheme_root</td>\n","      <td>153</td>\n","    </tr>\n","    <tr>\n","      <th>32</th>\n","      <td>Test_10_vowel_diacritic</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>33</th>\n","      <td>Test_11_consonant_diacritic</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>34</th>\n","      <td>Test_11_grapheme_root</td>\n","      <td>21</td>\n","    </tr>\n","    <tr>\n","      <th>35</th>\n","      <td>Test_11_vowel_diacritic</td>\n","      <td>2</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                         row_id  target\n","0    Test_0_consonant_diacritic       0\n","1          Test_0_grapheme_root       3\n","2        Test_0_vowel_diacritic       1\n","3    Test_1_consonant_diacritic       0\n","4          Test_1_grapheme_root      93\n","5        Test_1_vowel_diacritic       2\n","6    Test_2_consonant_diacritic       0\n","7          Test_2_grapheme_root      19\n","8        Test_2_vowel_diacritic       0\n","9    Test_3_consonant_diacritic       0\n","10         Test_3_grapheme_root     115\n","11       Test_3_vowel_diacritic       0\n","12   Test_4_consonant_diacritic       0\n","13         Test_4_grapheme_root      55\n","14       Test_4_vowel_diacritic       4\n","15   Test_5_consonant_diacritic       0\n","16         Test_5_grapheme_root     115\n","17       Test_5_vowel_diacritic       2\n","18   Test_6_consonant_diacritic       5\n","19         Test_6_grapheme_root     150\n","20       Test_6_vowel_diacritic       9\n","21   Test_7_consonant_diacritic       0\n","22         Test_7_grapheme_root     137\n","23       Test_7_vowel_diacritic       7\n","24   Test_8_consonant_diacritic       0\n","25         Test_8_grapheme_root     119\n","26       Test_8_vowel_diacritic       9\n","27   Test_9_consonant_diacritic       0\n","28         Test_9_grapheme_root     133\n","29       Test_9_vowel_diacritic      10\n","30  Test_10_consonant_diacritic       4\n","31        Test_10_grapheme_root     153\n","32      Test_10_vowel_diacritic       1\n","33  Test_11_consonant_diacritic       0\n","34        Test_11_grapheme_root      21\n","35      Test_11_vowel_diacritic       2"]},"metadata":{"tags":[]},"execution_count":11}]}]}