{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"submission.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOFM1ZzcGSVfRx95Bb9dWZw"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"vGJGsJfsggSr","colab_type":"code","outputId":"e8d2232e-5547-4aca-f319-2e094e82c01f","executionInfo":{"status":"ok","timestamp":1583201161610,"user_tz":-540,"elapsed":20062,"user":{"displayName":"Ryo Terajima","photoUrl":"","userId":"06768188657916272381"}},"colab":{"base_uri":"https://localhost:8080/","height":139}},"source":["from google.colab import drive\n","drive.mount('/content/gdrive/')\n","%cd gdrive/My\\ Drive/KKB-kaggle/bengaliai-cv19/notebooks"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive/\n","/content/gdrive/My Drive/KKB-kaggle/bengaliai-cv19/notebooks\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"U67CYVMTg7tE","colab_type":"code","colab":{}},"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load in \n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the \"../input/\" directory.\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# Any results you write to the current directory are saved as output."],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RU66nFHqgj7K","colab_type":"code","colab":{}},"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import recall_score\n","import cv2\n","# from tqdm.auto import tqdm\n","import copy\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader\n","from torchvision import datasets, transforms, utils\n","from torch.autograd import Variable\n","import torch.optim as optim\n","import torchvision\n","# from torchsummary import summary\n","import gc\n","\n","dataset_dir = '../dataset'\n","# dataset_dir = '/kaggle/input/bengaliai-cv19'\n","train_df = pd.read_csv(dataset_dir + '/train.csv')\n","test_df = pd.read_csv(dataset_dir + '/test.csv')\n","class_map_df = pd.read_csv(dataset_dir + '/class_map.csv')\n","sample_sub_df = pd.read_csv(dataset_dir + '/sample_submission.csv')\n","\n","# 前処理を関数にまとめた\n","def resize(X, out_height=64, out_width=64):\n","    print('Resizing raw image... / 前処理実行中…')\n","    # resized = {} # 前処理された画像が格納されるリスト\n","    resized = np.zeros((len(X), out_height * out_width))\n","\n","    for i in range(len(X)):\n","        image = X.iloc[[i]].values.reshape(HEIGHT, WIDTH)\n","        _, thresh = cv2.threshold(image, 30, 255, cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)\n","        contours, _ = cv2.findContours(thresh, cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)[-2:]\n","        left = 1000\n","        right = -1\n","        top = 1000\n","        bottom = -1\n","\n","        for cnt in contours:\n","            x,y,w,h = cv2.boundingRect(cnt)\n","            left = min(x, left)\n","            right = max(x+w, right)\n","            top = min(y, top)\n","            bottom = max(y+h, bottom)\n","\n","        roi = image[top:bottom, left:right]\n","        resized_roi = cv2.resize(roi, (out_height, out_width),interpolation=cv2.INTER_AREA)\n","        resized[i] = resized_roi.reshape(-1)\n","\n","    # print(len(resized))\n","\n","    return resized\n","\n","# PyTorch式のデータセットクラスを定義\n","\n","class MyDataset(torch.utils.data.Dataset):\n","\n","    def __init__(self, X, Y, transform=None):\n","        self.transform = transform\n","        self.X = X\n","        self.Y = Y\n","\n","    def __len__(self):\n","        return len(self.X)\n","\n","    def __getitem__(self, idx):\n","        out_data = self.X[idx].reshape(1,64,64)\n","        out_data = torch.tensor(out_data, dtype=torch.float)\n","\n","        #root_label = torch.tensor(self.Y[0][idx], dtype=torch.long)\n","        #vowel_label = torch.tensor(self.Y[1][idx], dtype=torch.long)\n","        #cons_label = torch.tensor(self.Y[2][idx], dtype=torch.long)\n","\n","        root_label = torch.tensor(np.argmax(self.Y[0][idx]), dtype=torch.long)\n","        vowel_label = torch.tensor(np.argmax(self.Y[1][idx]), dtype=torch.long)\n","        cons_label = torch.tensor(np.argmax(self.Y[2][idx]), dtype=torch.long)\n","\n","        if self.transform:\n","            out_data = self.transform(out_data)\n","\n","        return out_data, root_label, vowel_label, cons_label\n","\n","def try_gpu(e):\n","    if torch.cuda.is_available():\n","        return e.cuda()\n","    return e\n","\n","class model(nn.Module):\n","    def __init__(self):\n","        super(model, self).__init__()\n","\n","        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)\n","        self.conv2 = nn.Conv2d(32, 32, 3, padding=1)\n","        self.conv3 = nn.Conv2d(32, 32, 3, padding=1)\n","        self.conv4 = nn.Conv2d(32, 32, 3, padding=1)\n","        self.b1 = nn.BatchNorm2d(32, momentum=0.15)     \n","        self.pool = nn.MaxPool2d(2,2)\n","        self.conv5 = nn.Conv2d(32, 32, 5, padding=2)\n","        self.conv5_dropout = nn.Dropout(p=0.3)\n","\n","        self.conv6 = nn.Conv2d(32, 64, 3, padding=1)\n","        self.conv7 = nn.Conv2d(64, 64, 3, padding=1)\n","        self.conv8 = nn.Conv2d(64, 64, 3, padding=1)\n","        self.conv9 = nn.Conv2d(64, 64, 3, padding=1)\n","        self.b2 = nn.BatchNorm2d(64, momentum=0.15)\n","        # self.pool = nn.MaxPool2d(2,2) same as upper pool\n","        self.conv10 = nn.Conv2d(64, 64, 5, padding=2)\n","        self.b3 = nn.BatchNorm2d(64, momentum=0.15)\n","        self.conv10_dropout = nn.Dropout(p=0.3)\n","\n","        self.conv11 = nn.Conv2d(64, 128, 3, padding=1)\n","        self.conv12 = nn.Conv2d(128, 128, 3, padding=1)\n","        self.conv13 = nn.Conv2d(128, 128, 3, padding=1)\n","        self.conv14 = nn.Conv2d(128, 128, 3, padding=1)\n","        self.b4 = nn.BatchNorm2d(128, momentum=0.15)\n","        # self.pool = nn.MaxPool2d(2,2) same as upper pool\n","        self.conv15 = nn.Conv2d(128, 128, 5, padding=2)\n","        self.b5 = nn.BatchNorm2d(128, momentum=0.15)\n","        self.conv15_dropout = nn.Dropout(p=0.3)\n","\n","        self.conv16 = nn.Conv2d(128, 256, 3, padding=1)\n","        self.conv17 = nn.Conv2d(256, 256, 3, padding=1)\n","        self.conv18 = nn.Conv2d(256, 256, 3, padding=1)\n","        self.conv19 = nn.Conv2d(256, 256, 3, padding=1)\n","        self.b6 = nn.BatchNorm2d(256, momentum=0.15)\n","        # self.pool = nn.MaxPool2d(2,2) same as upper pool\n","        self.conv20 = nn.Conv2d(256, 256, 5, padding=2)\n","        self.b7 = nn.BatchNorm2d(256, momentum=0.15)\n","        self.conv20_dropout = nn.Dropout(p=0.3)\n","\n","        self.dense1 = nn.Linear(256*4*4, 1024)\n","        self.dense1_dropout = nn.Dropout(p=0.3) # +relu\n","        self.dense2 = nn.Linear(1024, 512) # +relu\n","\n","        self.head_root = nn.Linear(512, 168) # + softmax\n","        self.head_vowel = nn.Linear(512, 11) # + softmax\n","        self.head_consonant = nn.Linear(512, 7) # + softmax\n","\n","    def forward(self, x):\n","        x = F.relu(self.conv1(x))\n","        x = F.relu(self.conv2(x))\n","        x = F.relu(self.conv3(x))\n","        x = self.pool(F.relu(self.b1(self.conv4(x))))\n","        x = F.relu(self.conv5(x))\n","        x = self.conv5_dropout(x)\n","        \n","        x = F.relu(self.conv6(x))\n","        x = F.relu(self.conv7(x))\n","        x = F.relu(self.conv8(x))\n","        x = self.pool(F.relu(self.b2(self.conv9(x))))\n","        x = F.relu(self.b3(self.conv10(x)))\n","        x = self.conv10_dropout(x)\n","\n","        x = F.relu(self.conv11(x))\n","        x = F.relu(self.conv12(x))\n","        x = F.relu(self.conv13(x))\n","        x = self.pool(F.relu(self.b4(self.conv14(x))))\n","        x = F.relu(self.b5(self.conv15(x)))\n","        x = self.conv15_dropout(x)\n","\n","        x = F.relu(self.conv16(x))\n","        x = F.relu(self.conv17(x))\n","        x = F.relu(self.conv18(x))\n","        x = self.pool(F.relu(self.b6(self.conv19(x))))\n","        x = F.relu(self.b7(self.conv20(x)))\n","        x = self.conv20_dropout(x)\n","\n","        x = x.view(-1, 256*4*4)\n","        x = F.relu(self.dense1(x))\n","        x = F.relu(self.dense2(x))\n","        \n","        head_root = self.head_root(x)\n","        head_vowel = self.head_vowel(x)\n","        head_consonant = self.head_consonant(x)\n","\n","        return head_root, head_vowel, head_consonant # not sure..\n","\n","model = model()\n","model = try_gpu(model)\n","\n","criterion1 = nn.CrossEntropyLoss() \n","#criterion2 = nn.CrossEntropyLoss() \n","#criterion3 = nn.CrossEntropyLoss() \n","optimizer = optim.Adam(model.parameters())\n","\n","def train(model, epoch, train_loader):\n","    model.train()\n","    print(f'Epoch number {epoch}')\n","    for i, data in enumerate(train_loader, 0):\n","        inputs, root_l, vowel_l, consonant_l = data\n","        inputs, root_l, vowel_l, consonant_l = Variable(inputs), Variable(root_l), Variable(vowel_l), Variable(consonant_l)\n","        inputs, root_l, vowel_l, consonant_l = try_gpu(inputs), try_gpu(root_l), try_gpu(vowel_l), try_gpu(consonant_l)\n","        optimizer.zero_grad()\n","        root_o, vowel_o, consonant_o = model(inputs)\n","        loss1 = criterion1(root_o, root_l)\n","        loss2 = criterion1(vowel_o, vowel_l)\n","        loss3 = criterion1(consonant_o, consonant_l)\n","        (loss1+loss2+loss3).backward()\n","        optimizer.step()\n","        # if i % 500 == 0:\n","        #     print(\"epoch{} root {}/{}  loss: {}\".format(epoch, i*len(inputs), len(train_loader)*len(inputs), loss1.data))\n","        #     print(\"epoch{} vowel {}/{}  loss: {}\".format(epoch, i*len(inputs), len(train_loader)*len(inputs), loss2.data))\n","        #     print(\"epoch{} consonant {}/{}  loss: {}\".format(epoch, i*len(inputs), len(train_loader)*len(inputs), loss3.data))\n","\n","def test(model, test_loader):\n","    model.eval()\n","\n","    size = len(test_loader.dataset)\n","    pred_r, pred_v, pred_c = np.zeros(size), np.zeros(size), np.zeros(size)\n","    true_r, true_v, true_c = np.zeros(size), np.zeros(size), np.zeros(size)\n","    index = 0\n","    \n","    for data in test_loader:\n","        inputs, root_l, vowel_l, consonant_l = data\n","        inputs, root_l, vowel_l, consonant_l = Variable(inputs), Variable(root_l), Variable(vowel_l), Variable(consonant_l)\n","        inputs, root_l, vowel_l, consonant_l = try_gpu(inputs), try_gpu(root_l), try_gpu(vowel_l), try_gpu(consonant_l)\n","        \n","        root_o, vowel_o, consonant_o = model(inputs) \n","        root_pred, vowel_pred, consonant_pred = torch.max(root_o.data,1)[1], torch.max(vowel_o.data,1)[1], torch.max(consonant_o.data,1)[1]\n","        for i in range(inputs.size(0)):\n","            pred_r[index] = root_pred[i]\n","            pred_v[index] = vowel_pred[i]\n","            pred_c[index] = consonant_pred[i]\n","            true_r[index] = root_l[i]\n","            true_v[index] = vowel_l[i]\n","            true_c[index] = consonant_l[i]\n","            index += 1\n","\n","    recall_r = recall_score(true_r, pred_r, average='macro')\n","    recall_v = recall_score(true_v, pred_v, average='macro')\n","    recall_c = recall_score(true_c, pred_c, average='macro')\n","    final_score = (2.*recall_r + recall_v + recall_c) / 4.\n","\n","    print(f'Root Recall: {recall_r:.5f}')\n","    print(f'Vowel Recall: {recall_v:.5f}')\n","    print(f'Consonant Recall: {recall_c:.5f}')\n","    print(f'Score: {final_score:.5f}')\n"," \n","\n","\n","    # total_r, total_v, total_c = 0,0,0\n","    # correct_r, correct_v, correct_c = 0,0,0\n","\n","    # for data in test_loader:\n","    #     inputs, root_l, vowel_l, consonant_l = data\n","    #     inputs, root_l, vowel_l, consonant_l = Variable(inputs), Variable(root_l), Variable(vowel_l), Variable(consonant_l)\n","    #     inputs, root_l, vowel_l, consonant_l = try_gpu(inputs), try_gpu(root_l), try_gpu(vowel_l), try_gpu(consonant_l)\n","        \n","    #     root_o, vowel_o, consonant_o = model(inputs)\n","    #     root_pred, vowel_pred, consonant_pred = torch.max(root_o.data,1)[1], torch.max(vowel_o.data,1)[1], torch.max(consonant_o.data,1)[1]\n","    #     total_r += root_l.size(0)\n","    #     correct_r += (root_pred == root_l).sum()\n","    #     total_v += vowel_l.size(0)\n","    #     correct_v += (vowel_pred == vowel_l).sum()\n","    #     total_c += consonant_l.size(0)\n","    #     correct_c += (consonant_pred == consonant_l).sum()\n","\n","    # print(\"root Accuracy {}/{} {:.2f}%\".format(correct_r, total_r, 100.0*correct_r/total_r))\n","    # print(\"vowel Accuracy {}/{} {:.2f}%\".format(correct_v, total_v, 100.0*correct_v/total_v))\n","    # print(\"consonant Accuracy {}/{} {:.2f}%\".format(correct_c, total_c, 100.0*correct_c/total_c))\n"," \n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"FYS78a00X7UE","colab_type":"code","outputId":"18aded9c-0c84-44c3-875f-7e21d155b7e0","executionInfo":{"status":"ok","timestamp":1583201780329,"user_tz":-540,"elapsed":619217,"user":{"displayName":"Ryo Terajima","photoUrl":"","userId":"06768188657916272381"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["# 訓練ループ / Training Loop\n","\n","###############\n","# True なら Cross Validation を実施する\n","# Kaggle に提出するときは False にしてください\n","do_validation = True\n","\n","# True なら submission.csv を生成する\n","create_submission = False\n","\n","val_perc = 0.2  # validation set の割合（クロスバリデーション）\n","epochs = 1\n","\n","###############\n","\n","for parq_i in range(4):\n","    print('=============================')\n","    print(f'Parquet {parq_i} の訓練を開始')\n","\n","    train_df_with_img = pd.merge(pd.read_parquet(dataset_dir + f'/train_image_data_{parq_i}.parquet'), train_df, on='image_id').drop(['image_id'], axis=1)\n","\n","    HEIGHT = 137\n","    WIDTH = 236\n","\n","    X_train = train_df_with_img.drop(columns=['grapheme_root', 'vowel_diacritic', 'consonant_diacritic', 'grapheme'])\n","\n","    # X_train_resized = pd.DataFrame(resize(X_train)).T / 255.0   # 値を0~1におさめる\n","    # X_train_resized = X_train_resized.values\n","    X_train_resized = resize(X_train) / 255.0\n","\n","    # メモリ節約\n","    del X_train\n","\n","    # PyTorchのデータセットクラスを作る前に、ラベルの情報も整備\n","    # 1-of-K符号化とか、One Hot Encodingとか呼ばれる方法でラベルをつくる\n","\n","    # 注：　PyTorchでは、ラベルはOne Hotじゃなくて良いことが判明したので、結局 MyDataset で元のラベルに戻している\n","\n","    Y_train_root = pd.get_dummies(train_df_with_img['grapheme_root']).values\n","    Y_train_vowel = pd.get_dummies(train_df_with_img['vowel_diacritic']).values\n","    Y_train_cons = pd.get_dummies(train_df_with_img['consonant_diacritic']).values\n","\n","    Y_train = [Y_train_root, Y_train_vowel, Y_train_cons]\n","\n","    trainval_dataset = MyDataset(X_train_resized, Y_train)\n","\n","    if do_validation:\n","        n_samples = len(trainval_dataset)\n","        train_size = int(len(trainval_dataset)*(1.0 - val_perc))\n","        val_size = n_samples - train_size\n","        print(f'train size: {train_size}, validation size: {val_size}')\n","\n","        train_dataset, val_dataset = torch.utils.data.random_split(trainval_dataset, [train_size, val_size])\n","\n","        train_loader = DataLoader(dataset=train_dataset,batch_size=32, shuffle=True, num_workers=4)\n","        val_loader = DataLoader(dataset=val_dataset, batch_size=32, num_workers=0)\n","\n","        for i in range(1,epochs+1):\n","            train(model, i, train_loader)\n","            test(model, val_loader)\n","\n","        # メモリ節約\n","        del train_df_with_img\n","        del train_dataset\n","        del val_dataset\n","        del trainval_dataset\n","        del train_loader\n","        del val_loader\n","        del X_train_resized\n","        gc.collect()\n","\n","    else:\n","        n_samples = len(trainval_dataset)\n","\n","        train_loader = DataLoader(dataset=trainval_dataset,\n","                            batch_size=32, shuffle=True, num_workers=4)\n","\n","        for i in range(1,epochs+1):\n","            train(model, i, train_loader)\n","\n","        # メモリ節約\n","        del train_df_with_img\n","        del trainval_dataset\n","        del train_loader\n","        del X_train_resized\n","        gc.collect()\n","\n","    \n","# 提出ファイルの準備\n","\n","target=[]\n","row_id=[] # row_id place holder\n","\n","for parq_i in range(4):\n","    df_test_img = pd.read_parquet(dataset_dir + f'/test_image_data_{parq_i}.parquet')\n","    # df_test_img = pd.read_parquet(dataset_dir + f'/train_image_data_{parq_i}.parquet') # Error Check!\n","    df_test_img.set_index('image_id', inplace=True)\n","\n","    # X_test_resized = resize(df_test_img)\n","    # X_test_resized = pd.DataFrame(X_test_resized).T / 255.0   # 値を0~1におさめる\n","    # X_test_resized = X_test_resized.values.reshape(-1, 1, 64, 64)\n","\n","    X_test_resized = resize(df_test_img).reshape(-1, 1, 64, 64) / 255.0\n","\n","    test_inputs = torch.tensor(X_test_resized, dtype=torch.float)\n","    test_inputs = Variable(test_inputs)\n","    test_inputs = try_gpu(test_inputs)\n","\n","    del X_test_resized\n","    gc.collect()\n","    \n","# テストデータをいっぺんに入れるとメモリが足りないので変更！\n","#     root_o, vowel_o, consonant_o = model(test_inputs)\n","#     root_pred, vowel_pred, consonant_pred = torch.max(root_o.data,1)[1], torch.max(vowel_o.data,1)[1], torch.max(consonant_o.data,1)[1]\n","#     if torch.cuda.is_available():\n","#         root_pred, vowel_pred, consonant_pred = root_pred.to(torch.device(\"cpu\")), vowel_pred.to(torch.device(\"cpu\")), consonant_pred.to(torch.device(\"cpu\"))\n","#\n","#     root_pred, vowel_pred, consonant_pred = root_pred.numpy(), vowel_pred.numpy(), consonant_pred.numpy()\n","#\n","#     for k, id in enumerate(df_test_img.index.values):\n","#         row_id.append(id+'_consonant_diacritic')\n","#         target.append(consonant_pred[k])\n","#         row_id.append(id+'_grapheme_root')\n","#         target.append(root_pred[k])\n","#         row_id.append(id+'_vowel_diacritic')\n","#         target.append(vowel_pred[k])\n","\n","    for k, id in enumerate(df_test_img.index.values):\n","        data = test_inputs[k].reshape(1,1,64,64)\n","        root_o, vowel_o, consonant_o = model(data)\n","        root_pred, vowel_pred, consonant_pred = torch.max(root_o.data,1)[1], torch.max(vowel_o.data,1)[1], torch.max(consonant_o.data,1)[1]\n","        if torch.cuda.is_available():\n","            root_pred, vowel_pred, consonant_pred = root_pred.to(torch.device(\"cpu\")), vowel_pred.to(torch.device(\"cpu\")), consonant_pred.to(torch.device(\"cpu\"))\n","\n","        root_pred, vowel_pred, consonant_pred = root_pred.item(), vowel_pred.item(), consonant_pred.item()\n","        \n","        row_id.append(id+'_consonant_diacritic')\n","        target.append(consonant_pred)\n","        row_id.append(id+'_grapheme_root')\n","        target.append(root_pred)\n","        row_id.append(id+'_vowel_diacritic')\n","        target.append(vowel_pred)\n","    \n","    del df_test_img\n","    gc.collect()\n","\n","df_sample = pd.DataFrame(\n","    {\n","        'row_id': row_id,\n","        'target':target\n","    },\n","    columns = ['row_id','target'] \n",")\n","\n","if create_submission:\n","    df_sample.to_csv('submission.csv',index=False)\n","\n","df_sample.head(36)"],"execution_count":4,"outputs":[{"output_type":"stream","text":["=============================\n","Parquet 0 の訓練を開始\n","Resizing raw image... / 前処理実行中…\n","train size: 40168, validation size: 10042\n","Epoch number 1\n","Root Recall: 0.02365\n","Vowel Recall: 0.52524\n","Consonant Recall: 0.27644\n","Score: 0.21224\n","=============================\n","Parquet 1 の訓練を開始\n","Resizing raw image... / 前処理実行中…\n","train size: 40168, validation size: 10042\n","Epoch number 1\n","Root Recall: 0.07804\n","Vowel Recall: 0.73291\n","Consonant Recall: 0.51194\n","Score: 0.35023\n","=============================\n","Parquet 2 の訓練を開始\n","Resizing raw image... / 前処理実行中…\n","Root Recall: 0.18697\n","Vowel Recall: 0.77831\n","Consonant Recall: 0.56872\n","Score: 0.43024\n","=============================\n","Parquet 3 の訓練を開始\n","Resizing raw image... / 前処理実行中…\n","train size: 40168, validation size: 10042\n","Epoch number 1\n","Root Recall: 0.31852\n","Vowel Recall: 0.79264\n","Consonant Recall: 0.74351\n","Score: 0.54330\n","Resizing raw image... / 前処理実行中…\n","Resizing raw image... / 前処理実行中…\n","Resizing raw image... / 前処理実行中…\n","Resizing raw image... / 前処理実行中…\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>row_id</th>\n","      <th>target</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Test_0_consonant_diacritic</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Test_0_grapheme_root</td>\n","      <td>70</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Test_0_vowel_diacritic</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Test_1_consonant_diacritic</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Test_1_grapheme_root</td>\n","      <td>139</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>Test_1_vowel_diacritic</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>Test_2_consonant_diacritic</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>Test_2_grapheme_root</td>\n","      <td>19</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>Test_2_vowel_diacritic</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>Test_3_consonant_diacritic</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>Test_3_grapheme_root</td>\n","      <td>112</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>Test_3_vowel_diacritic</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>Test_4_consonant_diacritic</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>Test_4_grapheme_root</td>\n","      <td>53</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>Test_4_vowel_diacritic</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>Test_5_consonant_diacritic</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>Test_5_grapheme_root</td>\n","      <td>115</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>Test_5_vowel_diacritic</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>Test_6_consonant_diacritic</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>Test_6_grapheme_root</td>\n","      <td>147</td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>Test_6_vowel_diacritic</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>21</th>\n","      <td>Test_7_consonant_diacritic</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>22</th>\n","      <td>Test_7_grapheme_root</td>\n","      <td>94</td>\n","    </tr>\n","    <tr>\n","      <th>23</th>\n","      <td>Test_7_vowel_diacritic</td>\n","      <td>7</td>\n","    </tr>\n","    <tr>\n","      <th>24</th>\n","      <td>Test_8_consonant_diacritic</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>25</th>\n","      <td>Test_8_grapheme_root</td>\n","      <td>133</td>\n","    </tr>\n","    <tr>\n","      <th>26</th>\n","      <td>Test_8_vowel_diacritic</td>\n","      <td>7</td>\n","    </tr>\n","    <tr>\n","      <th>27</th>\n","      <td>Test_9_consonant_diacritic</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>28</th>\n","      <td>Test_9_grapheme_root</td>\n","      <td>23</td>\n","    </tr>\n","    <tr>\n","      <th>29</th>\n","      <td>Test_9_vowel_diacritic</td>\n","      <td>10</td>\n","    </tr>\n","    <tr>\n","      <th>30</th>\n","      <td>Test_10_consonant_diacritic</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>31</th>\n","      <td>Test_10_grapheme_root</td>\n","      <td>103</td>\n","    </tr>\n","    <tr>\n","      <th>32</th>\n","      <td>Test_10_vowel_diacritic</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>33</th>\n","      <td>Test_11_consonant_diacritic</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>34</th>\n","      <td>Test_11_grapheme_root</td>\n","      <td>156</td>\n","    </tr>\n","    <tr>\n","      <th>35</th>\n","      <td>Test_11_vowel_diacritic</td>\n","      <td>2</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                         row_id  target\n","0    Test_0_consonant_diacritic       0\n","1          Test_0_grapheme_root      70\n","2        Test_0_vowel_diacritic       0\n","3    Test_1_consonant_diacritic       0\n","4          Test_1_grapheme_root     139\n","5        Test_1_vowel_diacritic       2\n","6    Test_2_consonant_diacritic       5\n","7          Test_2_grapheme_root      19\n","8        Test_2_vowel_diacritic       0\n","9    Test_3_consonant_diacritic       0\n","10         Test_3_grapheme_root     112\n","11       Test_3_vowel_diacritic       0\n","12   Test_4_consonant_diacritic       1\n","13         Test_4_grapheme_root      53\n","14       Test_4_vowel_diacritic       4\n","15   Test_5_consonant_diacritic       0\n","16         Test_5_grapheme_root     115\n","17       Test_5_vowel_diacritic       2\n","18   Test_6_consonant_diacritic       5\n","19         Test_6_grapheme_root     147\n","20       Test_6_vowel_diacritic       1\n","21   Test_7_consonant_diacritic       0\n","22         Test_7_grapheme_root      94\n","23       Test_7_vowel_diacritic       7\n","24   Test_8_consonant_diacritic       0\n","25         Test_8_grapheme_root     133\n","26       Test_8_vowel_diacritic       7\n","27   Test_9_consonant_diacritic       0\n","28         Test_9_grapheme_root      23\n","29       Test_9_vowel_diacritic      10\n","30  Test_10_consonant_diacritic       0\n","31        Test_10_grapheme_root     103\n","32      Test_10_vowel_diacritic       1\n","33  Test_11_consonant_diacritic       0\n","34        Test_11_grapheme_root     156\n","35      Test_11_vowel_diacritic       2"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"id":"n9LF7y4rafmo","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}