{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"train.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","authorship_tag":"ABX9TyOFF8WK0HYatY0zn39ZXFqr"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"0e17e8e6c3ec4e57b782b23b792ea472":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_690b41a16d8b438e870078397987ef53","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_94a06e830e8847e292afbc9e789c11cd","IPY_MODEL_55237545a1cc499c9faaeb1fff156155"]}},"690b41a16d8b438e870078397987ef53":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"94a06e830e8847e292afbc9e789c11cd":{"model_module":"@jupyter-widgets/controls","model_name":"IntProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_9238783e0dba4b49bd7a235a85993dc7","_dom_classes":[],"description":"","_model_name":"IntProgressModel","bar_style":"success","max":50210,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":50210,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_7e87365820554ab79d9b3df68b477dcc"}},"55237545a1cc499c9faaeb1fff156155":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_4f170fa33318446d80b91f4c011e05ae","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100% 50210/50210 [00:41&lt;00:00, 1196.36it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_8d490ff8ee8c4bc18c1f43f1046a601a"}},"9238783e0dba4b49bd7a235a85993dc7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"7e87365820554ab79d9b3df68b477dcc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"4f170fa33318446d80b91f4c011e05ae":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"8d490ff8ee8c4bc18c1f43f1046a601a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"id":"chbe6y5itQWb","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":122},"outputId":"f49207b2-180e-4e5a-e984-c6ac993c85cd","executionInfo":{"status":"ok","timestamp":1581920194650,"user_tz":-540,"elapsed":23547,"user":{"displayName":"정범준","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBOM61BGwey2npNQfr0e2LCuA9eQMwGWK2paLHa=s64","userId":"01726864658215807628"}}},"source":["from google.colab import drive\n","drive.mount('/content/gdrive/')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"96bpSLzftc6e","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"9a469fb8-581b-46fe-bd19-f5bea0e65212","executionInfo":{"status":"ok","timestamp":1581920195013,"user_tz":-540,"elapsed":4255,"user":{"displayName":"정범준","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBOM61BGwey2npNQfr0e2LCuA9eQMwGWK2paLHa=s64","userId":"01726864658215807628"}}},"source":["%cd gdrive/My\\ Drive/KKB-kaggle/bengaliai-cv19/notebooks"],"execution_count":2,"outputs":[{"output_type":"stream","text":["/content/gdrive/My Drive/KKB-kaggle/bengaliai-cv19/notebooks\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"0IRGRcectnRM","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":375,"referenced_widgets":["0e17e8e6c3ec4e57b782b23b792ea472","690b41a16d8b438e870078397987ef53","94a06e830e8847e292afbc9e789c11cd","55237545a1cc499c9faaeb1fff156155","9238783e0dba4b49bd7a235a85993dc7","7e87365820554ab79d9b3df68b477dcc","4f170fa33318446d80b91f4c011e05ae","8d490ff8ee8c4bc18c1f43f1046a601a"]},"outputId":"adbe2816-a0d6-49b6-dcfa-32ab1b1b011f","executionInfo":{"status":"ok","timestamp":1581920264950,"user_tz":-540,"elapsed":72507,"user":{"displayName":"정범준","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBOM61BGwey2npNQfr0e2LCuA9eQMwGWK2paLHa=s64","userId":"01726864658215807628"}}},"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import cv2\n","from tqdm.auto import tqdm\n","import copy\n","import torch\n","\n","dataset_dir = '../dataset'\n","train_df = pd.read_csv(dataset_dir + '/train.csv')\n","test_df = pd.read_csv(dataset_dir + '/test.csv')\n","class_map_df = pd.read_csv(dataset_dir + '/class_map.csv')\n","sample_sub_df = pd.read_csv(dataset_dir + '/sample_submission.csv')\n","\n","# データの形状\n","print(f'Size of training data: {train_df.shape}')\n","print(f'Size of test data: {test_df.shape}')\n","print(f'Size of class map: {class_map_df.shape}')\n","\n","# 各grapheme_root, consonant_diacritic, vowel_diacriticが何回出現しているのかを調べたい\n","# それぞれのgrapheme_root, consonant_diacritic, vowel_diacriticについての情報をまとめるDataFrameを作成する\n","\n","root_df = class_map_df[class_map_df['component_type']=='grapheme_root']\n","print(f'Size: {root_df.shape}')\n","cons_df = class_map_df[class_map_df['component_type']=='consonant_diacritic'].reset_index(drop=True)\n","print(f'Size: {cons_df.shape}')\n","vowel_df = class_map_df[class_map_df['component_type']=='vowel_diacritic'].reset_index(drop=True)\n","print(f'Size: {vowel_df.shape}')\n","\n","root_df.drop(columns=['component_type'], inplace=True)\n","cons_df.drop(columns=['component_type'], inplace=True)\n","vowel_df.drop(columns=['component_type'], inplace=True)\n","root_df['count'] = train_df.groupby(['grapheme_root']).size()\n","cons_df['count'] = train_df.groupby(['consonant_diacritic']).size()\n","vowel_df['count'] = train_df.groupby(['vowel_diacritic']).size()\n","\n","train_df_with_img = pd.merge(pd.read_parquet(dataset_dir + '/train_image_data_1.parquet'), train_df, on='image_id').drop(['image_id'], axis=1)\n","\n","HEIGHT = 137\n","WIDTH = 236\n","\n","X_train = train_df_with_img.drop(columns=['grapheme_root', 'vowel_diacritic', 'consonant_diacritic', 'grapheme'])\n","# 二値化\n","\n","image = X_train.iloc[[0]].values.reshape(HEIGHT, WIDTH)\n","_, thresh = cv2.threshold(image, 30, 255, cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU) # サンプルコードから持ってきたけど、cv2.THRESH_OTSUを使っているので30の意味は特になさそう？\n","\n","# 輪郭の抽出\n","\n","contours, _ = cv2.findContours(thresh, cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)[-2:]\n","# [-2:]の意味はわかりません、調査中です\n","\n","# すべての画像について、上記の処理を行う\n","RESIZE_H = 64\n","RESIZE_W = 64\n","\n","resized = {} # 前処理された画像が格納されるリスト\n","\n","for i in tqdm(range(len(X_train))):\n","  image = X_train.iloc[[i]].values.reshape(HEIGHT, WIDTH)\n","  _, thresh = cv2.threshold(image, 30, 255, cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)\n","  contours, _ = cv2.findContours(thresh, cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)[-2:]\n","  left = 1000\n","  right = -1\n","  top = 1000\n","  bottom = -1\n","\n","  for cnt in contours:\n","    x,y,w,h = cv2.boundingRect(cnt)\n","    left = min(x, left)\n","    right = max(x+w, right)\n","    top = min(y, top)\n","    bottom = max(y+h, bottom)\n","\n","  roi = image[top:bottom, left:right]\n","  resized_roi = cv2.resize(roi, (RESIZE_H, RESIZE_W),interpolation=cv2.INTER_AREA)\n","  resized[i] = resized_roi.reshape(-1)\n","\n","print(len(resized))\n"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Size of training data: (200840, 5)\n","Size of test data: (36, 3)\n","Size of class map: (186, 3)\n","Size: (168, 3)\n","Size: (7, 3)\n","Size: (11, 3)\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py:4117: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  errors=errors,\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"],"name":"stderr"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0e17e8e6c3ec4e57b782b23b792ea472","version_minor":0,"version_major":2},"text/plain":["HBox(children=(IntProgress(value=0, max=50210), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","50210\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"gxA5Oa4Uum-Q","colab_type":"code","colab":{}},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader\n","from torchvision import datasets, transforms, utils\n","from torch.autograd import Variable\n","import torch.optim as optim\n","import torchvision\n","from torchsummary import summary\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","X_train_resized = pd.DataFrame(resized).T / 255.0   # 値を0~1におさめる\n","X_train_resized = X_train_resized.values\n","\n","# PyTorchのデータセットクラスを作る前に、ラベルの情報も整備\n","# 1-of-K符号化とか、One Hot Encodingとか呼ばれる方法でラベルをつくる\n","\n","Y_train_root = pd.get_dummies(train_df_with_img['grapheme_root']).values\n","Y_train_vowel = pd.get_dummies(train_df_with_img['vowel_diacritic']).values\n","Y_train_cons = pd.get_dummies(train_df_with_img['consonant_diacritic']).values\n","\n","Y_train = [Y_train_root, Y_train_vowel, Y_train_cons]\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pm2h8s-n1Hfh","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"LXqulHRnvUr1","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":918},"outputId":"c8014712-9b4c-416a-d67b-11a3394bf2e9","executionInfo":{"status":"ok","timestamp":1581923753859,"user_tz":-540,"elapsed":1451,"user":{"displayName":"정범준","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBOM61BGwey2npNQfr0e2LCuA9eQMwGWK2paLHa=s64","userId":"01726864658215807628"}}},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader\n","from torchvision import datasets, transforms, utils\n","from torch.autograd import Variable\n","import torch.optim as optim\n","import torchvision\n","from torchsummary import summary\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","def try_gpu(e):\n","    if torch.cuda.is_available():\n","        return e.cuda()\n","    return e\n","\n","class model(nn.Module):\n","    def __init__(self):\n","        super(model, self).__init__()\n","\n","        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)\n","        self.conv2 = nn.Conv2d(32, 32, 3, padding=1)\n","        self.conv3 = nn.Conv2d(32, 32, 3, padding=1)\n","        self.conv4 = nn.Conv2d(32, 32, 3, padding=1)\n","        self.b1 = nn.BatchNorm2d(32, momentum=0.15)     \n","        self.pool = nn.MaxPool2d(2,2)\n","        self.conv5 = nn.Conv2d(32, 32, 5, padding=2)\n","        self.conv5_dropout = nn.Dropout(p=0.3)\n","\n","        self.conv6 = nn.Conv2d(32, 64, 3, padding=1)\n","        self.conv7 = nn.Conv2d(64, 64, 3, padding=1)\n","        self.conv8 = nn.Conv2d(64, 64, 3, padding=1)\n","        self.conv9 = nn.Conv2d(64, 64, 3, padding=1)\n","        self.b2 = nn.BatchNorm2d(64, momentum=0.15)\n","        # self.pool = nn.MaxPool2d(2,2) same as upper pool\n","        self.conv10 = nn.Conv2d(64, 64, 5, padding=2)\n","        self.b3 = nn.BatchNorm2d(64, momentum=0.15)\n","        self.conv10_dropout = nn.Dropout(p=0.3)\n","\n","        self.conv11 = nn.Conv2d(64, 128, 3, padding=1)\n","        self.conv12 = nn.Conv2d(128, 128, 3, padding=1)\n","        self.conv13 = nn.Conv2d(128, 128, 3, padding=1)\n","        self.conv14 = nn.Conv2d(128, 128, 3, padding=1)\n","        self.b4 = nn.BatchNorm2d(128, momentum=0.15)\n","        # self.pool = nn.MaxPool2d(2,2) same as upper pool\n","        self.conv15 = nn.Conv2d(128, 128, 5, padding=2)\n","        self.b5 = nn.BatchNorm2d(128, momentum=0.15)\n","        self.conv15_dropout = nn.Dropout(p=0.3)\n","\n","        self.conv16 = nn.Conv2d(128, 256, 3, padding=1)\n","        self.conv17 = nn.Conv2d(256, 256, 3, padding=1)\n","        self.conv18 = nn.Conv2d(256, 256, 3, padding=1)\n","        self.conv19 = nn.Conv2d(256, 256, 3, padding=1)\n","        self.b6 = nn.BatchNorm2d(256, momentum=0.15)\n","        # self.pool = nn.MaxPool2d(2,2) same as upper pool\n","        self.conv20 = nn.Conv2d(256, 256, 5, padding=2)\n","        self.b7 = nn.BatchNorm2d(256, momentum=0.15)\n","        self.conv20_dropout = nn.Dropout(p=0.3)\n","\n","        self.dense1 = nn.Linear(256*4*4, 1024)\n","        self.dense1_dropout = nn.Dropout(p=0.3) # +relu\n","        self.dense2 = nn.Linear(1024, 512) # +relu\n","\n","        self.head_root = nn.Linear(512, 168) # + softmax\n","        self.head_vowel = nn.Linear(512, 11) # + softmax\n","        self.head_consonant = nn.Linear(512, 7) # + softmax\n","\n","    def forward(self, x):\n","        x = F.relu(self.conv1(x))\n","        x = F.relu(self.conv2(x))\n","        x = F.relu(self.conv3(x))\n","        x = self.pool(F.relu(self.b1(self.conv4(x))))\n","        x = F.relu(self.conv5(x))\n","        x = self.conv5_dropout(x)\n","        \n","        x = F.relu(self.conv6(x))\n","        x = F.relu(self.conv7(x))\n","        x = F.relu(self.conv8(x))\n","        x = self.pool(F.relu(self.b2(self.conv9(x))))\n","        x = F.relu(self.b3(self.conv10(x)))\n","        x = self.conv10_dropout(x)\n","\n","        x = F.relu(self.conv11(x))\n","        x = F.relu(self.conv12(x))\n","        x = F.relu(self.conv13(x))\n","        x = self.pool(F.relu(self.b4(self.conv14(x))))\n","        x = F.relu(self.b5(self.conv15(x)))\n","        x = self.conv15_dropout(x)\n","\n","        x = F.relu(self.conv16(x))\n","        x = F.relu(self.conv17(x))\n","        x = F.relu(self.conv18(x))\n","        x = self.pool(F.relu(self.b6(self.conv19(x))))\n","        x = F.relu(self.b7(self.conv20(x)))\n","        x = self.conv20_dropout(x)\n","\n","        x = x.view(-1, 256*4*4)\n","        x = F.relu(self.dense1(x))\n","        x = F.relu(self.dense2(x))\n","        \n","        head_root = self.head_root(x)\n","        head_vowel = self.head_vowel(x)\n","        head_consonant = self.head_consonant(x)\n","\n","        return head_root, head_vowel, head_consonant # not sure..\n","\n","model = model()\n","model = try_gpu(model)\n","\n","criterion1 = nn.CrossEntropyLoss() \n","criterion2 = nn.CrossEntropyLoss() \n","criterion3 = nn.CrossEntropyLoss() \n","optimizer = optim.Adam(model.parameters())\n","\n","summary(model, (1, 64, 64))"],"execution_count":76,"outputs":[{"output_type":"stream","text":["----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1           [-1, 32, 64, 64]             320\n","            Conv2d-2           [-1, 32, 64, 64]           9,248\n","            Conv2d-3           [-1, 32, 64, 64]           9,248\n","            Conv2d-4           [-1, 32, 64, 64]           9,248\n","       BatchNorm2d-5           [-1, 32, 64, 64]              64\n","         MaxPool2d-6           [-1, 32, 32, 32]               0\n","            Conv2d-7           [-1, 32, 32, 32]          25,632\n","           Dropout-8           [-1, 32, 32, 32]               0\n","            Conv2d-9           [-1, 64, 32, 32]          18,496\n","           Conv2d-10           [-1, 64, 32, 32]          36,928\n","           Conv2d-11           [-1, 64, 32, 32]          36,928\n","           Conv2d-12           [-1, 64, 32, 32]          36,928\n","      BatchNorm2d-13           [-1, 64, 32, 32]             128\n","        MaxPool2d-14           [-1, 64, 16, 16]               0\n","           Conv2d-15           [-1, 64, 16, 16]         102,464\n","      BatchNorm2d-16           [-1, 64, 16, 16]             128\n","          Dropout-17           [-1, 64, 16, 16]               0\n","           Conv2d-18          [-1, 128, 16, 16]          73,856\n","           Conv2d-19          [-1, 128, 16, 16]         147,584\n","           Conv2d-20          [-1, 128, 16, 16]         147,584\n","           Conv2d-21          [-1, 128, 16, 16]         147,584\n","      BatchNorm2d-22          [-1, 128, 16, 16]             256\n","        MaxPool2d-23            [-1, 128, 8, 8]               0\n","           Conv2d-24            [-1, 128, 8, 8]         409,728\n","      BatchNorm2d-25            [-1, 128, 8, 8]             256\n","          Dropout-26            [-1, 128, 8, 8]               0\n","           Conv2d-27            [-1, 256, 8, 8]         295,168\n","           Conv2d-28            [-1, 256, 8, 8]         590,080\n","           Conv2d-29            [-1, 256, 8, 8]         590,080\n","           Conv2d-30            [-1, 256, 8, 8]         590,080\n","      BatchNorm2d-31            [-1, 256, 8, 8]             512\n","        MaxPool2d-32            [-1, 256, 4, 4]               0\n","           Conv2d-33            [-1, 256, 4, 4]       1,638,656\n","      BatchNorm2d-34            [-1, 256, 4, 4]             512\n","          Dropout-35            [-1, 256, 4, 4]               0\n","           Linear-36                 [-1, 1024]       4,195,328\n","           Linear-37                  [-1, 512]         524,800\n","           Linear-38                  [-1, 168]          86,184\n","           Linear-39                   [-1, 11]           5,643\n","           Linear-40                    [-1, 7]           3,591\n","================================================================\n","Total params: 9,733,242\n","Trainable params: 9,733,242\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.02\n","Forward/backward pass size (MB): 11.01\n","Params size (MB): 37.13\n","Estimated Total Size (MB): 48.16\n","----------------------------------------------------------------\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"VwE24sEN4vi_","colab_type":"code","colab":{}},"source":["# PyTorch式のデータセットクラスを定義\n","\n","class MyDataset(torch.utils.data.Dataset):\n","\n","    def __init__(self, X, Y, transform=None):\n","        self.transform = transform\n","        self.X = X\n","        self.Y = Y\n","\n","    def __len__(self):\n","        return len(self.X)\n","\n","    def __getitem__(self, idx):\n","        out_data = self.X[idx].reshape(1,64,64)\n","        out_data = torch.tensor(out_data, dtype=torch.float)\n","\n","        #root_label = torch.tensor(self.Y[0][idx], dtype=torch.long)\n","        #vowel_label = torch.tensor(self.Y[1][idx], dtype=torch.long)\n","        #cons_label = torch.tensor(self.Y[2][idx], dtype=torch.long)\n","\n","        root_label = torch.tensor(np.argmax(self.Y[0][idx]), dtype=torch.long)\n","        vowel_label = torch.tensor(np.argmax(self.Y[1][idx]), dtype=torch.long)\n","        cons_label = torch.tensor(np.argmax(self.Y[2][idx]), dtype=torch.long)\n","\n","        if self.transform:\n","            out_data = self.transform(out_data)\n","\n","        return out_data, root_label, vowel_label, cons_label\n","train_dataset = MyDataset(X_train_resized, Y_train)\n","\n","train_loader = DataLoader(dataset=train_dataset,\n","                         batch_size=32, shuffle=True, num_workers=4)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qcRW_YgYyeSf","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":935},"outputId":"dc59e1e6-079c-4c89-e2f7-6a65925a17c6","executionInfo":{"status":"ok","timestamp":1581924274426,"user_tz":-540,"elapsed":444180,"user":{"displayName":"정범준","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBOM61BGwey2npNQfr0e2LCuA9eQMwGWK2paLHa=s64","userId":"01726864658215807628"}}},"source":["# このセルはあとで変える\n","def train(model, epoch):\n","    model.train()\n","    for i, data in enumerate(train_loader, 0):\n","        inputs, root_l, vowel_l, consonant_l = data\n","        inputs, root_l, vowel_l, consonant_l = Variable(inputs), Variable(root_l), Variable(vowel_l), Variable(consonant_l)\n","        inputs, root_l, vowel_l, consonant_l = try_gpu(inputs), try_gpu(root_l), try_gpu(vowel_l), try_gpu(consonant_l)\n","        optimizer.zero_grad()\n","        root_o, vowel_o, consonant_o = model(inputs)\n","        loss1 = criterion1(root_o, root_l)\n","        loss2 = criterion1(vowel_o, vowel_l)\n","        loss3 = criterion1(consonant_o, consonant_l)\n","        (loss1+loss2+loss3).backward()\n","        optimizer.step()\n","        if i % 1000 == 0:\n","            print(\"epoch{} root {}/{}  loss: {}\".format(epoch, i*len(inputs), len(train_loader)*len(inputs), loss1.data))\n","            print(\"epoch{} vowel {}/{}  loss: {}\".format(epoch, i*len(inputs), len(train_loader)*len(inputs), loss2.data))\n","            print(\"epoch{} consonant {}/{}  loss: {}\".format(epoch, i*len(inputs), len(train_loader)*len(inputs), loss3.data))\n","'''\n","def test(model):\n","    model.eval()\n","    correct = 0\n","    total = 0\n","    for data in test_loader:\n","        inputs, root_l, vowel_l, consonant_l = data\n","        root_o, vowel_o, consonant_o = model(inputs)\n","        root_pred, vowel_pred, consonant_pred = torch.max(root_o.data,1)[1], torch.max(head_o.data,1)[1], torch.max(consonant_o.data,1)[1]\n","        total_r += root_l.size(0)\n","        correct_r += (root_pred == root_l).sum()\n","        total_v += vowel_l.size(0)\n","        correct_v += (vowel_pred == vowel_l).sum()\n","        total_c += consonant_l.size(0)\n","        correct_c += (consonant_pred == consonant_l).sum()\n","    print(\"root Accuracy {}/{} {:.2f}%\".format(correct_r, total_r, 100.0*correct_r/total_r))\n","    print(\"vowel Accuracy {}/{} {:.2f}%\".format(correct_v, total_v, 100.0*correct_v/total_v))\n","    print(\"consonant Accuracy {}/{} {:.2f}%\".format(correct_c, total_c, 100.0*correct_c/total_c))\n","'''   \n","for i in range(1,10):\n","    train(model, i)\n","    #test(model)"],"execution_count":80,"outputs":[{"output_type":"stream","text":["epoch1 root 0/50240  loss: 5.124733924865723\n","epoch1 vowel 0/50240  loss: 2.406491994857788\n","epoch1 consonant 0/50240  loss: 1.9935741424560547\n","epoch1 root 32000/50240  loss: 4.718846321105957\n","epoch1 vowel 32000/50240  loss: 1.2125563621520996\n","epoch1 consonant 32000/50240  loss: 0.9595699310302734\n","epoch2 root 0/50240  loss: 3.8137335777282715\n","epoch2 vowel 0/50240  loss: 0.7412190437316895\n","epoch2 consonant 0/50240  loss: 0.6826667189598083\n","epoch2 root 32000/50240  loss: 3.2792105674743652\n","epoch2 vowel 32000/50240  loss: 0.5723735690116882\n","epoch2 consonant 32000/50240  loss: 0.41541096568107605\n","epoch3 root 0/50240  loss: 2.6108126640319824\n","epoch3 vowel 0/50240  loss: 0.2290537804365158\n","epoch3 consonant 0/50240  loss: 0.4059421420097351\n","epoch3 root 32000/50240  loss: 2.394523859024048\n","epoch3 vowel 32000/50240  loss: 0.6749585270881653\n","epoch3 consonant 32000/50240  loss: 0.4918842315673828\n","epoch4 root 0/50240  loss: 2.4123904705047607\n","epoch4 vowel 0/50240  loss: 0.19201843440532684\n","epoch4 consonant 0/50240  loss: 0.42324650287628174\n","epoch4 root 32000/50240  loss: 2.01831316947937\n","epoch4 vowel 32000/50240  loss: 0.20101022720336914\n","epoch4 consonant 32000/50240  loss: 0.11959637701511383\n","epoch5 root 0/50240  loss: 1.9278723001480103\n","epoch5 vowel 0/50240  loss: 0.37858086824417114\n","epoch5 consonant 0/50240  loss: 0.2634364366531372\n","epoch5 root 32000/50240  loss: 1.5954939126968384\n","epoch5 vowel 32000/50240  loss: 0.21411554515361786\n","epoch5 consonant 32000/50240  loss: 0.11912551522254944\n","epoch6 root 0/50240  loss: 1.2472503185272217\n","epoch6 vowel 0/50240  loss: 0.17295978963375092\n","epoch6 consonant 0/50240  loss: 0.18841807544231415\n","epoch6 root 32000/50240  loss: 1.1142159700393677\n","epoch6 vowel 32000/50240  loss: 0.18958187103271484\n","epoch6 consonant 32000/50240  loss: 0.23207484185695648\n","epoch7 root 0/50240  loss: 1.2287883758544922\n","epoch7 vowel 0/50240  loss: 0.1942116618156433\n","epoch7 consonant 0/50240  loss: 0.5092428922653198\n","epoch7 root 32000/50240  loss: 1.2187265157699585\n","epoch7 vowel 32000/50240  loss: 0.2717083692550659\n","epoch7 consonant 32000/50240  loss: 0.08961731195449829\n","epoch8 root 0/50240  loss: 0.8789504766464233\n","epoch8 vowel 0/50240  loss: 0.055310867726802826\n","epoch8 consonant 0/50240  loss: 0.12318354099988937\n","epoch8 root 32000/50240  loss: 0.7145241498947144\n","epoch8 vowel 32000/50240  loss: 0.300926148891449\n","epoch8 consonant 32000/50240  loss: 0.06316564977169037\n","epoch9 root 0/50240  loss: 0.9879518151283264\n","epoch9 vowel 0/50240  loss: 0.14991934597492218\n","epoch9 consonant 0/50240  loss: 0.09092098474502563\n","epoch9 root 32000/50240  loss: 1.011646032333374\n","epoch9 vowel 32000/50240  loss: 0.1220012903213501\n","epoch9 consonant 32000/50240  loss: 0.05397248640656471\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"zWqiaKkFyzG0","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}