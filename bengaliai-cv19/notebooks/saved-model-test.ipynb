{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"saved-model-test.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPdfjad+A9Fs+fFCxtJezC6"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"kb1rg7LL7CrI","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":70},"outputId":"5abccd60-de32-4951-9982-c22b4918af2f","executionInfo":{"status":"ok","timestamp":1583249203964,"user_tz":-540,"elapsed":1120,"user":{"displayName":"Naoaki Kanazawa","photoUrl":"","userId":"02988158448965055485"}}},"source":["from google.colab import drive\n","drive.mount('/content/gdrive/')\n","%cd gdrive/My\\ Drive/KKB-kaggle/bengaliai-cv19/notebooks"],"execution_count":11,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive/; to attempt to forcibly remount, call drive.mount(\"/content/gdrive/\", force_remount=True).\n","[Errno 2] No such file or directory: 'gdrive/My Drive/KKB-kaggle/bengaliai-cv19/notebooks'\n","/content/gdrive/My Drive/KKB-kaggle/bengaliai-cv19/notebooks\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"d-wAWeoc7Jsv","colab_type":"code","colab":{}},"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load in \n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the \"../input/\" directory.\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# Any results you write to the current directory are saved as output."],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RFbyM-6K7TKk","colab_type":"code","colab":{}},"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import recall_score\n","import cv2\n","# from tqdm.auto import tqdm\n","import copy\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader, ConcatDataset\n","from torchvision import datasets, transforms, utils\n","from torch.autograd import Variable\n","import torch.optim as optim\n","import torchvision\n","import PIL\n","# from torchsummary import summary\n","import gc\n","\n","dataset_dir = '../dataset'\n","# dataset_dir = '/kaggle/input/bengaliai-cv19'\n","train_df = pd.read_csv(dataset_dir + '/train.csv')\n","test_df = pd.read_csv(dataset_dir + '/test.csv')\n","class_map_df = pd.read_csv(dataset_dir + '/class_map.csv')\n","sample_sub_df = pd.read_csv(dataset_dir + '/sample_submission.csv')\n","\n","model_dir = '../model'\n","# model_dir = '/kaggle/input/bengalimodels'\n","\n","# 前処理を関数にまとめた\n","def resize(X, out_height=64, out_width=64):\n","    print('Resizing raw image... / 前処理実行中…')\n","    # resized = {} # 前処理された画像が格納されるリスト\n","    resized = np.zeros((len(X), out_height * out_width))\n","\n","    for i in range(len(X)):\n","        image = X.iloc[[i]].values.reshape(HEIGHT, WIDTH)\n","        _, thresh = cv2.threshold(image, 30, 255, cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)\n","        contours, _ = cv2.findContours(thresh, cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)[-2:]\n","        left = 1000\n","        right = -1\n","        top = 1000\n","        bottom = -1\n","\n","        for cnt in contours:\n","            x,y,w,h = cv2.boundingRect(cnt)\n","            left = min(x, left)\n","            right = max(x+w, right)\n","            top = min(y, top)\n","            bottom = max(y+h, bottom)\n","\n","        roi = image[top:bottom, left:right]\n","        resized_roi = cv2.resize(roi, (out_height, out_width),interpolation=cv2.INTER_AREA)\n","        resized[i] = resized_roi.reshape(-1)\n","\n","    # print(len(resized))\n","\n","    return resized\n","\n","def try_gpu(e):\n","    if torch.cuda.is_available():\n","        return e.cuda()\n","    return e"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rAhXVShH7Twh","colab_type":"code","colab":{}},"source":["#resnet\n","def conv3x3(in_channels, out_channels, stride=1, groups=1, dilation=1):\n","     return nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride,\n","                      padding=dilation, groups=groups, bias=True,\n","                      dilation=dilation)\n","\n","def conv1x1(in_channels, out_channels, stride=1):\n","     return nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=True)\n","\n","class BasicBlock(nn.Module):\n","    #Implementation of Basic Building Block\n","\n","    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n","        super(BasicBlock, self).__init__()\n","\n","        self.conv1 = conv3x3(in_channels, out_channels, stride)\n","        self.bn1 = nn.BatchNorm2d(out_channels)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.conv2 = conv3x3(out_channels, out_channels)\n","        self.bn2 = nn.BatchNorm2d(out_channels)\n","        self.downsample = downsample\n","\n","    def forward(self, x):\n","        identity_x = x  # hold input for shortcut connection\n","\n","        out = self.conv1(x)\n","        out = self.bn1(out)\n","        out = self.relu(out)\n","\n","        out = self.conv2(out)\n","        out = self.bn2(out)\n","\n","        if self.downsample is not None:\n","            identity_x = self.downsample(x)\n","\n","        out += identity_x  # shortcut connection\n","        return self.relu(out)\n","\n","class ResidualLayer(nn.Module):\n","\n","    def __init__(self, num_blocks, in_channels, out_channels, block=BasicBlock):\n","        super(ResidualLayer, self).__init__()\n","        downsample = None\n","        if in_channels != out_channels:\n","            downsample = nn.Sequential(\n","                conv1x1(in_channels, out_channels),\n","                nn.BatchNorm2d(out_channels))\n","        self.first_block = block(in_channels, out_channels, downsample=downsample)\n","        self.blocks = nn.ModuleList(block(out_channels, out_channels) for _ in range(num_blocks))\n","\n","    def forward(self, x):\n","        out = self.first_block(x)\n","        for block in self.blocks:\n","            out = block(out)\n","        return out\n","\n","class model(nn.Module):\n","    def __init__(self):\n","        #resnet18の実装\n","        super(model, self).__init__()\n","        self.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3)\n","        self.bn1 = nn.BatchNorm2d(64)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n","        self.layer1 = ResidualLayer(2, in_channels=64, out_channels=64)\n","        self.layer2 = ResidualLayer(2, in_channels=64, out_channels=128)\n","        self.layer3 = ResidualLayer(2, in_channels=128, out_channels=256)\n","        self.layer4 = ResidualLayer(2, in_channels=256, out_channels=512)\n","        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n","        self.fc = nn.Linear(512, 512)\n","\n","        self.head_root = nn.Linear(512, 168) # + softmax\n","        self.head_vowel = nn.Linear(512, 11) # + softmax\n","        self.head_consonant = nn.Linear(512, 7) # + softmax\n","    \n","    def forward(self, x):\n","        x = self.conv1(x)\n","        x = self.bn1(x)\n","        x = self.relu(x)\n","        x = self.maxpool(x)\n","\n","        x = self.layer1(x)\n","        x = self.layer2(x)\n","        x = self.layer3(x)\n","        x = self.layer4(x)\n","\n","        x = self.avg_pool(x)\n","        x = x.view(x.size(0), -1)\n","        x = self.fc(x)\n","        \n","        head_root = self.head_root(x)\n","        head_vowel = self.head_vowel(x)\n","        head_consonant = self.head_consonant(x)\n","\n","        return head_root, head_vowel, head_consonant # not sure..\n","\n","model = model()\n","model = try_gpu(model)\n","\n","criterion1 = nn.CrossEntropyLoss() \n","#criterion2 = nn.CrossEntropyLoss() \n","#criterion3 = nn.CrossEntropyLoss() \n","optimizer = optim.Adam(model.parameters())"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Dtjq1nF_7qsJ","colab_type":"code","colab":{}},"source":["def train(model, epoch, train_loader):\n","    model.train()\n","    print(f'Epoch number {epoch}')\n","    for i, data in enumerate(train_loader, 0):\n","        inputs, root_l, vowel_l, consonant_l = data\n","        inputs, root_l, vowel_l, consonant_l = Variable(inputs), Variable(root_l), Variable(vowel_l), Variable(consonant_l)\n","        inputs, root_l, vowel_l, consonant_l = try_gpu(inputs), try_gpu(root_l), try_gpu(vowel_l), try_gpu(consonant_l)\n","        optimizer.zero_grad()\n","        root_o, vowel_o, consonant_o = model(inputs)\n","        loss1 = criterion1(root_o, root_l)\n","        loss2 = criterion1(vowel_o, vowel_l)\n","        loss3 = criterion1(consonant_o, consonant_l)\n","        (loss1+loss2+loss3).backward()\n","        optimizer.step()\n","        # if i % 500 == 0:\n","        #     print(\"epoch{} root {}/{}  loss: {}\".format(epoch, i*len(inputs), len(train_loader)*len(inputs), loss1.data))\n","        #     print(\"epoch{} vowel {}/{}  loss: {}\".format(epoch, i*len(inputs), len(train_loader)*len(inputs), loss2.data))\n","        #     print(\"epoch{} consonant {}/{}  loss: {}\".format(epoch, i*len(inputs), len(train_loader)*len(inputs), loss3.data))\n","\n","def test(model, test_loader):\n","    model.eval()\n","\n","    size = len(test_loader.dataset)\n","    pred_r, pred_v, pred_c = np.zeros(size), np.zeros(size), np.zeros(size)\n","    true_r, true_v, true_c = np.zeros(size), np.zeros(size), np.zeros(size)\n","    index = 0\n","    \n","    for data in test_loader:\n","        inputs, root_l, vowel_l, consonant_l = data\n","        inputs, root_l, vowel_l, consonant_l = Variable(inputs), Variable(root_l), Variable(vowel_l), Variable(consonant_l)\n","        inputs, root_l, vowel_l, consonant_l = try_gpu(inputs), try_gpu(root_l), try_gpu(vowel_l), try_gpu(consonant_l)\n","        \n","        root_o, vowel_o, consonant_o = model(inputs) \n","        root_pred, vowel_pred, consonant_pred = torch.max(root_o.data,1)[1], torch.max(vowel_o.data,1)[1], torch.max(consonant_o.data,1)[1]\n","        for i in range(inputs.size(0)):\n","            pred_r[index] = root_pred[i]\n","            pred_v[index] = vowel_pred[i]\n","            pred_c[index] = consonant_pred[i]\n","            true_r[index] = root_l[i]\n","            true_v[index] = vowel_l[i]\n","            true_c[index] = consonant_l[i]\n","            index += 1\n","\n","    recall_r = recall_score(true_r, pred_r, average='macro')\n","    recall_v = recall_score(true_v, pred_v, average='macro')\n","    recall_c = recall_score(true_c, pred_c, average='macro')\n","    final_score = (2.*recall_r + recall_v + recall_c) / 4.\n","\n","    print(f'Root Recall: {recall_r:.5f}')\n","    print(f'Vowel Recall: {recall_v:.5f}')\n","    print(f'Consonant Recall: {recall_c:.5f}')\n","    print(f'Score: {final_score:.5f}')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_jd3T2Kh71C9","colab_type":"code","colab":{}},"source":["# Augmentation のための transform を定義\n","\n","transform_crop = transforms.Compose([\n","                                transforms.ToPILImage(),\n","                                transforms.RandomResizedCrop((64,64), scale=(0.80, 0.90)),\n","                                transforms.ToTensor()\n","])\n","\n","\n","rotate_left = transforms.RandomAffine((-20, -10), fillcolor=255, resample=PIL.Image.BILINEAR)\n","rotate_right = transforms.RandomAffine((10, 20), fillcolor=255, resample=PIL.Image.BILINEAR)\n","transform_rotate = transforms.Compose([\n","                                       transforms.ToPILImage(),\n","                                       transforms.RandomChoice([rotate_left, rotate_right]),\n","                                       transforms.ToTensor()\n","])\n","\n","\n","class AddGaussianNoise(object):\n","    def __init__(self, mean=0., std=1.):\n","        self.std = std\n","        self.mean = mean\n","        \n","    def __call__(self, tensor):\n","        with_noise = tensor + torch.randn(tensor.size()) * self.std + self.mean\n","        return torch.max(torch.min(with_noise, torch.tensor(1.)), torch.tensor(0.))\n","    \n","    def __repr__(self):\n","        return self.__class__.__name__ + '(mean={0}, std={1})'.format(self.mean, self.std)\n","\n","transform_noise = AddGaussianNoise(0., 0.01)\n","\n","\n","# PyTorch式のデータセットクラスを定義\n","\n","class MyDataset(torch.utils.data.Dataset):\n","\n","    def __init__(self, X, Y, transform=None):\n","        self.transform = transform\n","        self.X = X\n","        self.Y = Y\n","\n","    def __len__(self):\n","        return len(self.X)\n","\n","    def __getitem__(self, idx):\n","        out_data = self.X[idx].reshape(1,64,64)\n","        out_data = torch.tensor(out_data, dtype=torch.float)\n","\n","        #root_label = torch.tensor(self.Y[0][idx], dtype=torch.long)\n","        #vowel_label = torch.tensor(self.Y[1][idx], dtype=torch.long)\n","        #cons_label = torch.tensor(self.Y[2][idx], dtype=torch.long)\n","\n","        root_label = torch.tensor(np.argmax(self.Y[0][idx]), dtype=torch.long)\n","        vowel_label = torch.tensor(np.argmax(self.Y[1][idx]), dtype=torch.long)\n","        cons_label = torch.tensor(np.argmax(self.Y[2][idx]), dtype=torch.long)\n","\n","        if self.transform:\n","            out_data = self.transform(out_data)\n","\n","        return out_data, root_label, vowel_label, cons_label\n","\n","\n","class MakeSubset(torch.utils.data.Dataset):\n","\n","    def __init__(self, dataset, transform=None):\n","        self.dataset = dataset\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.dataset)\n","\n","    def __getitem__(self, idx):\n","        out_data, root_label, vowel_label, cons_label = self.dataset[idx]\n","\n","        if self.transform:\n","            out_data = self.transform(out_data)\n","\n","        return out_data, root_label, vowel_label, cons_label"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"L7MYGYwJ7niU","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"b85d94ba-2723-4e22-ffa8-226930b98250","executionInfo":{"status":"ok","timestamp":1583249217387,"user_tz":-540,"elapsed":2553,"user":{"displayName":"Naoaki Kanazawa","photoUrl":"","userId":"02988158448965055485"}}},"source":["model.load_state_dict(torch.load(model_dir + '/resnet_5epochs_val_saved_weights.pth'))"],"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"code","metadata":{"id":"tXWLEBoh7dxT","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":647},"outputId":"24271b24-d481-42ed-8262-59277d50ff7c","executionInfo":{"status":"ok","timestamp":1583249777489,"user_tz":-540,"elapsed":558234,"user":{"displayName":"Naoaki Kanazawa","photoUrl":"","userId":"02988158448965055485"}}},"source":["# 訓練ループ / Training Loop\n","\n","###############\n","# True なら Cross Validation を実施する\n","# Kaggle に提出するときは False にしてください\n","do_validation = True\n","\n","# True なら submission.csv を生成する\n","create_submission = False\n","\n","val_perc = 1  # validation set の割合（クロスバリデーション）\n","epochs = 1\n","\n","###############\n","\n","for parq_i in range(4):\n","    print('=============================')\n","    print(f'Parquet {parq_i} の訓練を開始')\n","\n","    train_df_with_img = pd.merge(pd.read_parquet(dataset_dir + f'/train_image_data_{parq_i}.parquet'), train_df, on='image_id').drop(['image_id'], axis=1)\n","\n","    HEIGHT = 137\n","    WIDTH = 236\n","\n","    X_train = train_df_with_img.drop(columns=['grapheme_root', 'vowel_diacritic', 'consonant_diacritic', 'grapheme'])\n","\n","    # X_train_resized = pd.DataFrame(resize(X_train)).T / 255.0   # 値を0~1におさめる\n","    # X_train_resized = X_train_resized.values\n","    X_train_resized = resize(X_train) / 255.0\n","\n","    # メモリ節約\n","    del X_train\n","\n","    # PyTorchのデータセットクラスを作る前に、ラベルの情報も整備\n","    # 1-of-K符号化とか、One Hot Encodingとか呼ばれる方法でラベルをつくる\n","\n","    # 注：　PyTorchでは、ラベルはOne Hotじゃなくて良いことが判明したので、結局 MyDataset で元のラベルに戻している\n","\n","    Y_train_root = pd.get_dummies(train_df_with_img['grapheme_root']).values\n","    Y_train_vowel = pd.get_dummies(train_df_with_img['vowel_diacritic']).values\n","    Y_train_cons = pd.get_dummies(train_df_with_img['consonant_diacritic']).values\n","\n","    Y_train = [Y_train_root, Y_train_vowel, Y_train_cons]\n","\n","    trainval_dataset = MyDataset(X_train_resized, Y_train)\n","\n","    del X_train_resized\n","    del train_df_with_img\n","    gc.collect()\n","\n","    if do_validation:\n","        n_samples = len(trainval_dataset)\n","        train_size = int(len(trainval_dataset)*(1.0 - val_perc))\n","        val_size = n_samples - train_size\n","        print(f'train size: {train_size}, validation size: {val_size}')\n","\n","        train_dataset, val_dataset = torch.utils.data.random_split(trainval_dataset, [train_size, val_size])\n","\n","        aug_dataset = MakeSubset(train_dataset)\n","        # train_subset2 = MakeSubset(train_dataset, transform=transform_crop)\n","        # train_subset3 = MakeSubset(train_dataset, transform=transform_rotate)\n","        # train_subset4 = MakeSubset(train_dataset, transform=transform_noise)\n","\n","        del trainval_dataset\n","        del train_dataset\n","        gc.collect()\n","\n","        # aug_dataset = ConcatDataset([train_subset1, train_subset2, train_subset3, train_subset4])\n","        \n","        # del train_subset1, train_subset2, train_subset3, train_subset4\n","        # gc.collect()\n","\n","        # train_loader = DataLoader(dataset=aug_dataset, batch_size=32, shuffle=True, num_workers=4)\n","        val_loader = DataLoader(dataset=val_dataset, batch_size=32, num_workers=4)\n","        print(f'train size (after augmentation): {len(aug_dataset)}, validation size: {len(val_dataset)}')\n","\n","        for i in range(1,epochs+1):\n","            # train(model, i, train_loader)\n","            test(model, val_loader)\n","\n","        # メモリ節約\n","        del aug_dataset\n","        del val_dataset\n","        # del train_loader\n","        del val_loader\n","        gc.collect()\n","\n","    else:\n","        train_subset1 = MakeSubset(trainval_dataset)\n","        train_subset2 = MakeSubset(trainval_dataset, transform=transform_crop)\n","        train_subset3 = MakeSubset(trainval_dataset, transform=transform_rotate)\n","        train_subset4 = MakeSubset(trainval_dataset, transform=transform_noise)\n","\n","        del trainval_dataset\n","        gc.collect()\n","\n","        aug_dataset = ConcatDataset([train_subset1, train_subset2, train_subset3, train_subset4])\n","        \n","        del train_subset1, train_subset2, train_subset3, train_subset4\n","        gc.collect()\n","\n","        train_loader = DataLoader(dataset=aug_dataset, batch_size=32, shuffle=True, num_workers=4)\n","        print(f'train size (after augmentation): {len(aug_dataset)}')\n","\n","        for i in range(1,epochs+1):\n","            train(model, i, train_loader)\n","\n","        # メモリ節約\n","        del aug_dataset\n","        del train_loader\n","        gc.collect()"],"execution_count":18,"outputs":[{"output_type":"stream","text":["=============================\n","Parquet 0 の訓練を開始\n","Resizing raw image... / 前処理実行中…\n","train size: 0, validation size: 50210\n","train size (after augmentation): 0, validation size: 50210\n","Root Recall: 0.92546\n","Vowel Recall: 0.97682\n","Consonant Recall: 0.96116\n","Score: 0.94722\n","=============================\n","Parquet 1 の訓練を開始\n","Resizing raw image... / 前処理実行中…\n","train size: 0, validation size: 50210\n","train size (after augmentation): 0, validation size: 50210\n","Root Recall: 0.93119\n","Vowel Recall: 0.98028\n","Consonant Recall: 0.96265\n","Score: 0.95133\n","=============================\n","Parquet 2 の訓練を開始\n","Resizing raw image... / 前処理実行中…\n","train size: 0, validation size: 50210\n","train size (after augmentation): 0, validation size: 50210\n","Root Recall: 0.94201\n","Vowel Recall: 0.98166\n","Consonant Recall: 0.96731\n","Score: 0.95825\n","=============================\n","Parquet 3 の訓練を開始\n","Resizing raw image... / 前処理実行中…\n","train size: 0, validation size: 50210\n","train size (after augmentation): 0, validation size: 50210\n","Root Recall: 0.97964\n","Vowel Recall: 0.99395\n","Consonant Recall: 0.98890\n","Score: 0.98553\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Qd60dEwj8Gue","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"c69fd70a-0d64-4168-a4a4-5a04dd368443","executionInfo":{"status":"ok","timestamp":1583249807923,"user_tz":-540,"elapsed":3413,"user":{"displayName":"Naoaki Kanazawa","photoUrl":"","userId":"02988158448965055485"}}},"source":["model.load_state_dict(torch.load(model_dir + '/resnet_5epochs_saved_weights.pth'))"],"execution_count":19,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"code","metadata":{"id":"65xTm1Rb8bQC","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":647},"outputId":"38a5b37d-b11c-46ba-8b78-b733ff143bf0","executionInfo":{"status":"ok","timestamp":1583250388946,"user_tz":-540,"elapsed":579738,"user":{"displayName":"Naoaki Kanazawa","photoUrl":"","userId":"02988158448965055485"}}},"source":["# 訓練ループ / Training Loop\n","\n","###############\n","# True なら Cross Validation を実施する\n","# Kaggle に提出するときは False にしてください\n","do_validation = True\n","\n","# True なら submission.csv を生成する\n","create_submission = False\n","\n","val_perc = 1  # validation set の割合（クロスバリデーション）\n","epochs = 1\n","\n","###############\n","\n","for parq_i in range(4):\n","    print('=============================')\n","    print(f'Parquet {parq_i} の訓練を開始')\n","\n","    train_df_with_img = pd.merge(pd.read_parquet(dataset_dir + f'/train_image_data_{parq_i}.parquet'), train_df, on='image_id').drop(['image_id'], axis=1)\n","\n","    HEIGHT = 137\n","    WIDTH = 236\n","\n","    X_train = train_df_with_img.drop(columns=['grapheme_root', 'vowel_diacritic', 'consonant_diacritic', 'grapheme'])\n","\n","    # X_train_resized = pd.DataFrame(resize(X_train)).T / 255.0   # 値を0~1におさめる\n","    # X_train_resized = X_train_resized.values\n","    X_train_resized = resize(X_train) / 255.0\n","\n","    # メモリ節約\n","    del X_train\n","\n","    # PyTorchのデータセットクラスを作る前に、ラベルの情報も整備\n","    # 1-of-K符号化とか、One Hot Encodingとか呼ばれる方法でラベルをつくる\n","\n","    # 注：　PyTorchでは、ラベルはOne Hotじゃなくて良いことが判明したので、結局 MyDataset で元のラベルに戻している\n","\n","    Y_train_root = pd.get_dummies(train_df_with_img['grapheme_root']).values\n","    Y_train_vowel = pd.get_dummies(train_df_with_img['vowel_diacritic']).values\n","    Y_train_cons = pd.get_dummies(train_df_with_img['consonant_diacritic']).values\n","\n","    Y_train = [Y_train_root, Y_train_vowel, Y_train_cons]\n","\n","    trainval_dataset = MyDataset(X_train_resized, Y_train)\n","\n","    del X_train_resized\n","    del train_df_with_img\n","    gc.collect()\n","\n","    if do_validation:\n","        n_samples = len(trainval_dataset)\n","        train_size = int(len(trainval_dataset)*(1.0 - val_perc))\n","        val_size = n_samples - train_size\n","        print(f'train size: {train_size}, validation size: {val_size}')\n","\n","        train_dataset, val_dataset = torch.utils.data.random_split(trainval_dataset, [train_size, val_size])\n","\n","        aug_dataset = MakeSubset(train_dataset)\n","        # train_subset2 = MakeSubset(train_dataset, transform=transform_crop)\n","        # train_subset3 = MakeSubset(train_dataset, transform=transform_rotate)\n","        # train_subset4 = MakeSubset(train_dataset, transform=transform_noise)\n","\n","        del trainval_dataset\n","        del train_dataset\n","        gc.collect()\n","\n","        # aug_dataset = ConcatDataset([train_subset1, train_subset2, train_subset3, train_subset4])\n","        \n","        # del train_subset1, train_subset2, train_subset3, train_subset4\n","        # gc.collect()\n","\n","        # train_loader = DataLoader(dataset=aug_dataset, batch_size=32, shuffle=True, num_workers=4)\n","        val_loader = DataLoader(dataset=val_dataset, batch_size=32, num_workers=4)\n","        print(f'train size (after augmentation): {len(aug_dataset)}, validation size: {len(val_dataset)}')\n","\n","        for i in range(1,epochs+1):\n","            # train(model, i, train_loader)\n","            test(model, val_loader)\n","\n","        # メモリ節約\n","        del aug_dataset\n","        del val_dataset\n","        # del train_loader\n","        del val_loader\n","        gc.collect()\n","\n","    else:\n","        train_subset1 = MakeSubset(trainval_dataset)\n","        train_subset2 = MakeSubset(trainval_dataset, transform=transform_crop)\n","        train_subset3 = MakeSubset(trainval_dataset, transform=transform_rotate)\n","        train_subset4 = MakeSubset(trainval_dataset, transform=transform_noise)\n","\n","        del trainval_dataset\n","        gc.collect()\n","\n","        aug_dataset = ConcatDataset([train_subset1, train_subset2, train_subset3, train_subset4])\n","        \n","        del train_subset1, train_subset2, train_subset3, train_subset4\n","        gc.collect()\n","\n","        train_loader = DataLoader(dataset=aug_dataset, batch_size=32, shuffle=True, num_workers=4)\n","        print(f'train size (after augmentation): {len(aug_dataset)}')\n","\n","        for i in range(1,epochs+1):\n","            train(model, i, train_loader)\n","\n","        # メモリ節約\n","        del aug_dataset\n","        del train_loader\n","        gc.collect()"],"execution_count":20,"outputs":[{"output_type":"stream","text":["=============================\n","Parquet 0 の訓練を開始\n","Resizing raw image... / 前処理実行中…\n","train size: 0, validation size: 50210\n","train size (after augmentation): 0, validation size: 50210\n","Root Recall: 0.93776\n","Vowel Recall: 0.97827\n","Consonant Recall: 0.97305\n","Score: 0.95671\n","=============================\n","Parquet 1 の訓練を開始\n","Resizing raw image... / 前処理実行中…\n","train size: 0, validation size: 50210\n","train size (after augmentation): 0, validation size: 50210\n","Root Recall: 0.94288\n","Vowel Recall: 0.98289\n","Consonant Recall: 0.97317\n","Score: 0.96045\n","=============================\n","Parquet 2 の訓練を開始\n","Resizing raw image... / 前処理実行中…\n","train size: 0, validation size: 50210\n","train size (after augmentation): 0, validation size: 50210\n","Root Recall: 0.95444\n","Vowel Recall: 0.98599\n","Consonant Recall: 0.98331\n","Score: 0.96955\n","=============================\n","Parquet 3 の訓練を開始\n","Resizing raw image... / 前処理実行中…\n","train size: 0, validation size: 50210\n","train size (after augmentation): 0, validation size: 50210\n","Root Recall: 0.99629\n","Vowel Recall: 0.99778\n","Consonant Recall: 0.99865\n","Score: 0.99725\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"GCtoH0Z6Ct6p","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}